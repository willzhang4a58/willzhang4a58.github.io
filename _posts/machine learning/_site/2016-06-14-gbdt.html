<ul id="markdown-toc">
  <li><a href="#notations" id="markdown-toc-notations">1. Notations</a></li>
  <li><a href="#regression-tree-and-ensemble" id="markdown-toc-regression-tree-and-ensemble">2. Regression Tree and Ensemble</a></li>
  <li><a href="#objective" id="markdown-toc-objective">3. Objective</a></li>
  <li><a href="#boosting" id="markdown-toc-boosting">4. Boosting</a></li>
  <li><a href="#refine-the-objective" id="markdown-toc-refine-the-objective">5. Refine the Objective</a></li>
  <li><a href="#greedy-learning-of-the-tree" id="markdown-toc-greedy-learning-of-the-tree">6. Greedy Learning of the Tree</a></li>
  <li><a href="#pruning-and-regularization" id="markdown-toc-pruning-and-regularization">7. Pruning and Regularization</a></li>
  <li><a href="#recap" id="markdown-toc-recap">8. Recap</a></li>
</ul>

<h2 id="notations">1. Notations</h2>

<p>第<script type="math/tex">i</script>个训练样本<script type="math/tex">x_i \in R^d</script>，其真实的Label为<script type="math/tex">y_i</script>，模型的预测Label为<script type="math/tex">\hat{y}_i</script></p>

<h2 id="regression-tree-and-ensemble">2. Regression Tree and Ensemble</h2>

<p>回归树与决策树一样，非叶节点有决策规则，此外，其叶节点包含一个分数</p>

<p>将单一一个回归树视为一个函数，也就是<script type="math/tex">f(x)</script></p>

<p>将多个回归树的结果Ensemble后也就是最后的<script type="math/tex">\hat{y}_i</script></p>

<p>比较常见的一种做法就是</p>

<p>\begin{align}
\hat{y}_i=\sum_{k}f_k(x_i)
\end{align}</p>

<h2 id="objective">3. Objective</h2>

<p>首先确定Model，假设我们有<script type="math/tex">k</script>个树</p>

<p>\begin{align}
\hat{y}_i = \sum_{k=1}^Kf_k(x_i) 
\end{align}</p>

<p>设定</p>

<p>\begin{align}
Obj=\sum_{i=1}^nl(y_i,\hat{y}_i) + \sum_{k=1}^K \Omega(f_k)
\end{align}</p>

<p>上式中的第一项称为training loss，第二项则是complexity of the trees，也就是一般意义上的regularization</p>

<p>比较常见的training loss函数如</p>

<p>\begin{align}
l(y_i,\hat{y}_i) &amp;= (y_i - \hat{y}_i)^2 
\end{align}</p>

<h2 id="boosting">4. Boosting</h2>

<p>初始时<script type="math/tex">\hat{y}_i^{(0)} = 0</script>
每轮训练一颗树，加入到模型中
\begin{align}
\hat{y}_i^{(1)} &amp;= \hat{y}_i^{(0)} + f_1(x_i) \\
\hat{y}_i^{(2)} &amp;= \hat{y}_i^{(1)} + f_2(x_i) \\
&amp;…… \\
\hat{y}_i^{(t)} &amp;= \hat{y}_i^{(t-1)} + f_t(x_i) = \sum_{k=1}^tf_k(x_i)
\end{align}</p>

<p>在第<script type="math/tex">t</script>轮时，我们需要确定如何构造第<script type="math/tex">t</script>颗树，也就是<script type="math/tex">f_t(x)</script></p>

<p>在第<script type="math/tex">t</script>轮时，模型的预测结果为<script type="math/tex">\hat{y}_i^{(t)}=\hat{y}_i^{(t-1)} + f_t(x_i)</script></p>

<p>因此</p>

<p>\begin{align}
Obj^{(t)}= &amp; \sum_{i=1}^nl(y_i,\hat{y}_i^{(t)}) + \sum_{i=1}^t \Omega(f_k) \\
= &amp; \sum_{i=1}^nl(y_i,\hat{y}_i^{(t-1)} + f_t(x_i)) + \sum_{i=1}^t \Omega(f_k) \\
= &amp; \sum_{i=1}^nl(y_i,\hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t) + const
\end{align}</p>

<h2 id="refine-the-objective">5. Refine the Objective</h2>

<p>回忆泰勒展开</p>

<p>\begin{align}
f(x+\Delta x) \approx f(x) + f’(x)\Delta x + \frac{1}{2}f’'(x)\Delta x^2
\end{align}</p>

<p>得到</p>

<p>\begin{align}
Obj^{(t)} &amp;=\sum_{i=1}^nl(y_i,\hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t) + const \\
&amp;\approx \sum_{i=1}^n \left[ l(y_i,\hat{y}_i^{(t-1)}) + \frac{\partial l(y_i,\hat{y}_i^{(t-1)})}{\partial \hat{y}_i^{(t-1)}}f_t(x_i) +\frac{1}{2}\frac{\partial^2 l(y_i,\hat{y}_i^{(t-1)})}{\partial (\hat{y}_i^{(t-1)})^2}f_t^2(x_i) \right] + \Omega(f_t) + const \\
\end{align}</p>

<p>为了简化，令</p>

<p>\begin{align}
g_i &amp;= \frac{\partial l(y_i,\hat{y}_i^{(t-1)})}{\partial \hat{y}_i^{(t-1)}} \\
h_i &amp;= \frac{\partial^2 l(y_i,\hat{y}_i^{(t-1)})}{\partial (\hat{y}_i^{(t-1)})^2}
\end{align}</p>

<p>于是
\begin{align}
Obj^{(t)} 
&amp;\approx \sum_{i=1}^n\left[ l(y_i,\hat{y}_i^{(t-1)}) + g_if_t(x_i) +\frac{1}{2}h_if_t^2(x_i)\right] + \Omega(f_t) + const \\
&amp;= \sum_{i=1}^n\left[ g_if_t(x_i) +\frac{1}{2}h_if_t^2(x_i)\right] + \Omega(f_t) + const 
\end{align}</p>

<p>回归树的每个叶节点都与一个分数相关，因此可以将<script type="math/tex">f_t(x)</script>定义为</p>

<p>\begin{align}
f_t(x) = w_{q(x)},w\in R^T,q:R^d \rightarrow \{1,2,…,T\}
\end{align}</p>

<p><script type="math/tex">T</script>为该回归树的叶节点数量，<script type="math/tex">w</script>是树的叶节点对应的分数</p>

<p>树的复杂度，容易想到一个是叶节点数<script type="math/tex">T</script>，另一个是叶节点分数的大小</p>

<p>因此，可以定义（非唯一定义）</p>

<p>\begin{align}
\Omega(f_t)=\gamma T + \frac{1}{2}\lambda \sum_{j=1}^Tw_j^2
\end{align}</p>

<p>为了进一步化简，我们定义属于叶节点<script type="math/tex">j</script>的数据集合为</p>

<p>\begin{align}
I_j=\{i | q(x_i)=j\}
\end{align}</p>

<p>有</p>

<p>\begin{align}
Obj^{(t)} 
&amp;\approx \sum_{i=1}^n\left[ g_if_t(x_i) +\frac{1}{2}h_if_t^2(x_i)\right] + \Omega(f_t) + const \\
&amp;= \sum_{i=1}^n\left[ g_if_t(x_i) +\frac{1}{2}h_if_t^2(x_i)\right] + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^Tw_j^2 + const \\
&amp;= \sum_{i=1}^n\left[ g_iw_{q(x_i)} +\frac{1}{2}h_iw_{q(x_i)}^2\right] + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^Tw_j^2 + const \\
&amp;= \sum_{j=1}^T \left[ \left(\sum_{i\in I_j}g_i\right)w_j +\frac{1}{2}\left(\sum_{i \in I_j} h_i\right)w_j^2\right] + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^Tw_j^2 + const \\
&amp;= \sum_{j=1}^T \left[ \left(\sum_{i\in I_j}g_i\right)w_j +\frac{1}{2}\left(\lambda + \sum_{i \in I_j} h_i\right)w_j^2\right] + \gamma T  + const
\end{align}</p>

<p>为了符号简洁，定义</p>

<p>\begin{align}
G_j&amp;=\sum_{i\in I_j}g_i \\
H_j&amp;=\sum_{i\in I_j}h_i
\end{align}</p>

<p>有</p>

<p>\begin{align}
Obj^{(t)} \approx \sum_{j=1}^T \left[ G_jw_j +\frac{1}{2}\left(\lambda + H_j \right)w_j^2\right] + \gamma T  + const
\end{align}</p>

<p>假设<script type="math/tex">q(x)</script>固定，那么每个叶节点对上式的贡献互相独立</p>

<p>由二次函数性质，可以得到最优解为</p>

<p>\begin{align}
w_j^* &amp;=-\frac{G_j}{H_j+\lambda} \\
{Obj}^* &amp;= -\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda} + \gamma T
\end{align}</p>

<h2 id="greedy-learning-of-the-tree">6. Greedy Learning of the Tree</h2>

<p>在说贪心之前，先说一种非常朴素的方法，流程如下</p>

<ul>
  <li>遍历树的所有可能的<script type="math/tex">q</script>，计算其在<script type="math/tex">q</script>固定的情况下最优的<script type="math/tex">Obj^*_q</script></li>
  <li>得到最优的<script type="math/tex">Obj^*=\mathop{argmin}_q Obj^*_q</script></li>
  <li>计算各个叶节点的分数<script type="math/tex">w_j^* =-\frac{G_j}{H_j+\lambda}</script></li>
</ul>

<p>这个方法显然是不可能实现的，接下来说贪心的方法</p>

<p>从根节点开始，分裂出子节点，假设分出两个子节点，这个过程中Objective的变化为</p>

<p>\begin{align}
Gain = \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} - \gamma
\end{align}</p>

<p>以此为依据选择最优的特征进行分裂</p>

<p>和普通的决策树相比，其分裂规则由损失函数推导得来</p>

<h2 id="pruning-and-regularization">7. Pruning and Regularization</h2>

<ul>
  <li>Pre-stopping
    <ul>
      <li>如果增益为负，提前结束</li>
    </ul>
  </li>
  <li>Post-Prunning
    <ul>
      <li>使树生长到最大高度后，迭代地剪掉负增益的叶节点</li>
    </ul>
  </li>
</ul>

<h2 id="recap">8. Recap</h2>

<p>总结一下GBDT的流程</p>

<ul>
  <li>每次迭代新增一棵树</li>
  <li>每次迭代开始时，计算所有的<script type="math/tex">g_i,h_i</script></li>
  <li>贪心构造新的树<script type="math/tex">f_t(x)</script></li>
  <li>将新树增加到模型中<script type="math/tex">\hat{y}^{(t)}=\hat{y}^{(t-1)} + f_t(x_i)</script>
    <ul>
      <li>通常，我们不会严格按照上式增加新树，而是<script type="math/tex">\hat{y}^{(t)}=\hat{y}^{(t-1)} + \epsilon f_t(x_i)</script>，其中的<script type="math/tex">\epsilon</script>通常设为<script type="math/tex">0.1</script>。这意味着我们每轮都给下次的迭代留下优化的机会，这通常能缓解过拟合的问题</li>
    </ul>
  </li>
</ul>
