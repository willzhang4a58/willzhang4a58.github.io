<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>willzhang4a58&#39;s Personal Website</title>
    <description>willzhang4a58&#39;s Personal Website</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 22 May 2016 21:52:09 +0800</pubDate>
    <lastBuildDate>Sun, 22 May 2016 21:52:09 +0800</lastBuildDate>
    <generator>Jekyll v3.1.6</generator>
    
      <item>
        <title>Bagging and Random Forest</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#bagging&quot; id=&quot;markdown-toc-bagging&quot;&gt;1. Bagging&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#random-forest&quot; id=&quot;markdown-toc-random-forest&quot;&gt;2. Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bagging&quot;&gt;1. Bagging&lt;/h2&gt;

&lt;p&gt;首先我们有m条训练数据集&lt;/p&gt;

&lt;p&gt;对这m条数据进行放回式采样，采样m次得到的m条数据用来训练一个模型&lt;/p&gt;

&lt;p&gt;重复t次（模型可能使用不同算法产生），就能得到t个模型&lt;/p&gt;

&lt;p&gt;对于单个样本，将t个模型对其的预测结果通过某种策略合并产生最终的预测结果&lt;/p&gt;

&lt;p&gt;需要注意到，放回式采样，会导致对于每个模型，其实都会有约36.8%的数据（称为“包外样本”）并没有参与训练，我们可以用这部分数据作为验证数据集&lt;/p&gt;

&lt;h2 id=&quot;random-forest&quot;&gt;2. Random Forest&lt;/h2&gt;

&lt;p&gt;随机森林是Bagging的一个扩展&lt;/p&gt;

&lt;p&gt;其t个模型都使用决策树，并且对决策树的训练过程做了一些修改&lt;/p&gt;

&lt;p&gt;普通的决策树模型在节点分裂是会选择一个最优的特征进行分裂&lt;/p&gt;

&lt;p&gt;假设当前分裂节点有n个特征可以选择&lt;/p&gt;

&lt;p&gt;随机森林首先会随机出大小为k（&lt;script type=&quot;math/tex&quot;&gt;k\leq n&lt;/script&gt;）的特征子集，随后再从中选择最优的特征进行分裂&lt;/p&gt;

&lt;p&gt;经验值&lt;script type=&quot;math/tex&quot;&gt;k=\log_2n&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Bagging策略中的包外样本还可用于对决策树进行剪枝&lt;/p&gt;
</description>
        <pubDate>Sun, 22 May 2016 05:16:01 +0800</pubDate>
        <link>/2016/05/22/random-forest/</link>
        <guid isPermaLink="true">/2016/05/22/random-forest/</guid>
        
        
        <category>Machine&amp;nbsp;Learning</category>
        
      </item>
    
      <item>
        <title>Decision Tree</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#intuition&quot; id=&quot;markdown-toc-intuition&quot;&gt;1. Intuition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;2. 构建流程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;3. 分裂规则&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;3.1 信息增益&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;3.2 增益率&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-4&quot; id=&quot;markdown-toc-section-4&quot;&gt;3.3 基尼指数&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;intuition&quot;&gt;1. Intuition&lt;/h2&gt;

&lt;p&gt;决策树是一种分类算法，其模型为树结构&lt;/p&gt;

&lt;p&gt;每个非叶节点代表一个测试，而每个叶节点都与一个类别相关&lt;/p&gt;

&lt;p&gt;其对单个样本的分类过程从最上的根节点开始，根据测试结果往下走，直到走到叶节点&lt;/p&gt;

&lt;p&gt;这样这个样本就被决策树预测了类别，一个例子如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;2. 构建流程&lt;/h2&gt;

&lt;p&gt;大部分决策树构建算法都是自顶向下构建的&lt;/p&gt;

&lt;p&gt;根节点处理全部的数据，并将其划分为多个子集，传给其子节点&lt;/p&gt;

&lt;p&gt;子节点处理其接收到的数据，再次划分，这样递归下去完成决策树的构建&lt;/p&gt;

&lt;p&gt;其单个节点的构建详细流程如下&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;接收本节点的数据D&lt;/li&gt;
  &lt;li&gt;if D中所有的数据都是一个类别
    &lt;ul&gt;
      &lt;li&gt;将本节点设定为叶节点，并将其类别标注为&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;的类别&lt;/li&gt;
      &lt;li&gt;结束&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;使用&lt;strong&gt;某种分裂规则&lt;/strong&gt;将D划分为若干个子集&lt;/li&gt;
  &lt;li&gt;对所有的子集
    &lt;ul&gt;
      &lt;li&gt;创建子树&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;某种分裂规则&lt;/strong&gt;是指选择某个特征，并以该特征的值作为划分数据子集的依据&lt;/p&gt;

&lt;p&gt;通常这种准则，应使得各个子集尽可能只有一个类别&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;3. 分裂规则&lt;/h2&gt;

&lt;p&gt;这里介绍三个比较常见的分裂依据，分别为信息增益、增益率、基尼指数&lt;/p&gt;

&lt;p&gt;设数据&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;中的类别有&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;个，第&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;个类别以&lt;script type=&quot;math/tex&quot;&gt;C_i&lt;/script&gt;表示&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;C_{i,D}&lt;/script&gt;是&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;中&lt;script type=&quot;math/tex&quot;&gt;C_i&lt;/script&gt;类数据的集合&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;3.1 信息增益&lt;/h3&gt;

&lt;p&gt;ID3使用信息增益作为分裂依据&lt;/p&gt;

&lt;p&gt;数据D的熵为&lt;/p&gt;

&lt;p&gt;\begin{align}
Entropy(D) = -\sum_{i=1}^mp_i\log_2(p_i)
\end{align}&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt;表示类别&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;在数据&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;中的概率，可以设定为&lt;/p&gt;

&lt;p&gt;\begin{align}p_i=\frac{|C_{i,D}|}{|D|}
\end{align}&lt;/p&gt;

&lt;p&gt;假定选择特征&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;对数据&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;进行划分&lt;/p&gt;

&lt;p&gt;先假定&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;是离散特征，在数据&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;中&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;的所有可能取值有&lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;个，第&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;个取值为&lt;script type=&quot;math/tex&quot;&gt;a_v&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;于是很自然地划分出&lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;个子集，第&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;个子集称为&lt;script type=&quot;math/tex&quot;&gt;D_i&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;划分后的熵可以定义为&lt;/p&gt;

&lt;p&gt;\begin{align}
Entropy_A(D) = \sum_{j=1}^v\frac{|D_j|}{|D|}Entropy(D_j)
\end{align}&lt;/p&gt;

&lt;p&gt;信息增益定义为&lt;/p&gt;

&lt;p&gt;\begin{align}
Gain(A) = Entropy(D) - Entropy_A(D)
\end{align}&lt;/p&gt;

&lt;p&gt;显然，&lt;script type=&quot;math/tex&quot;&gt;Gain(A)&lt;/script&gt;表示通过划分，不确定性减少的程度&lt;/p&gt;

&lt;p&gt;于是我们要选择使得信息增益最大的特征进行划分&lt;/p&gt;

&lt;p&gt;上面考虑的是A是离散值的情况下&lt;/p&gt;

&lt;p&gt;实际上，在A是连续值的情况下，在分裂时是选择一个阈值&lt;/p&gt;

&lt;p&gt;特征的值小于等于阈值的划分为一个子集，大于阈值的划分为另一个子集&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;3.2 增益率&lt;/h3&gt;

&lt;p&gt;ID3有一个缺点，倾向于划分出更多的子集。&lt;/p&gt;

&lt;p&gt;极端情况下，将数据D划分为一条数据一个子集，但是这种划分对于分类并没有用&lt;/p&gt;

&lt;p&gt;C4.5是ID3的后继者，引入一个参数用于改善这种缺点&lt;/p&gt;

&lt;p&gt;引入一个称为split information的值&lt;/p&gt;

&lt;p&gt;对特征A做划分的split information为&lt;/p&gt;

&lt;p&gt;\begin{align}
SplitInfo_A(D) = -\sum_{j=1}^v\frac{|D_j|}{|D|}\log_2\left(\frac{|D_j|}{|D|}\right)
\end{align}&lt;/p&gt;

&lt;p&gt;容易发现，这其实是个熵，划分出的子集个数越多，这个split information越大&lt;/p&gt;

&lt;p&gt;而增益率定义为&lt;/p&gt;

&lt;p&gt;\begin{align}
GrainRate(A) = \frac{Gain(A)}{SplitInfo_A(D)}
\end{align}&lt;/p&gt;

&lt;p&gt;选择最大增益率的特征进行分裂，这种方法就是C4.5&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;3.3 基尼指数&lt;/h3&gt;

&lt;p&gt;CART使用基尼指数作为划分依据&lt;/p&gt;

&lt;p&gt;基尼指数度量数据D的不纯度（数据纯意味这所有数据都是一个类别）&lt;/p&gt;

&lt;p&gt;\begin{align}
Gini(D) = 1 - \sum_{i=1}^mp_i^2
\end{align}&lt;/p&gt;

&lt;p&gt;CART对于离散值和连续值特征都是划分为两个子集&lt;/p&gt;

&lt;p&gt;设划分为两个子集&lt;script type=&quot;math/tex&quot;&gt;D_1&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;D_2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;划分后的基尼指数设定为&lt;/p&gt;

&lt;p&gt;\begin{align}
Gini_A(D) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)
\end{align}&lt;/p&gt;

&lt;p&gt;选择&lt;script type=&quot;math/tex&quot;&gt;Gini_A(D)&lt;/script&gt;最小的特征进行分裂，这种方法就是CART&lt;/p&gt;
</description>
        <pubDate>Fri, 20 May 2016 18:52:10 +0800</pubDate>
        <link>/2016/05/20/decision-tree/</link>
        <guid isPermaLink="true">/2016/05/20/decision-tree/</guid>
        
        
        <category>Machine&amp;nbsp;Learning</category>
        
      </item>
    
      <item>
        <title>LinUCB</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#ucb&quot; id=&quot;markdown-toc-ucb&quot;&gt;1. UCB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#linucb&quot; id=&quot;markdown-toc-linucb&quot;&gt;2. LinUCB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ucb&quot;&gt;1. UCB&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;/2016/05/16/mab&quot;&gt;上一篇文章&lt;/a&gt;中介绍了一种context-free的方法，&lt;script type=&quot;math/tex&quot;&gt;\epsilon-greedy&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;其能够很好地完成对未知商品的exploration，同时完成对奖励的exploitation。&lt;/p&gt;

&lt;p&gt;但是在后期，假如所有的商品都已经探索完毕，并且没有新商品加进来&lt;/p&gt;

&lt;p&gt;显然此时并不需要做exploration，&lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;概率对应部分的奖励就拿不到了&lt;/p&gt;

&lt;p&gt;在此，介绍一种新的方法，Upper Confidence Bound&lt;/p&gt;

&lt;p&gt;这里仅简单介绍一下基本思想&lt;/p&gt;

&lt;p&gt;假如有一个老虎机，玩了10次，奖励是2，那么可以认为其奖励均值是2，此时设定其均值的95%置信区间为[0,4]&lt;/p&gt;

&lt;p&gt;而另一个老虎机，玩了100词，奖励均值是3，那么可以设定其均值的95%置信区间为[2.9,3.1]&lt;/p&gt;

&lt;p&gt;UCB在此时的决策是选择置信区间上界最大的一个老虎机&lt;/p&gt;

&lt;p&gt;很容易发现&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于未知商品，尽管其均值可能很低，但是由于其不确定性会导致置信区间的上界会很大，从而触发exploration&lt;/li&gt;
  &lt;li&gt;对于已经很熟悉的商品，如果其均值很高，会触发exploitation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;UCB的难点在于如何设定置信上界，设定方法有很多种，这里不介绍&lt;/p&gt;

&lt;p&gt;设定的方法通常会满足如下性质&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;随着迭代轮数的增长，上界会远离均值&lt;/li&gt;
  &lt;li&gt;随着被选择次数的增长，上界会靠近均值&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;linucb&quot;&gt;2. LinUCB&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;/2016/05/16/mab&quot;&gt;上一篇文章&lt;/a&gt;中提到了Contextual Bandit，在Multi-Armed Bandit的基础上每次做决策时还会有一个特征向量&lt;/p&gt;

&lt;p&gt;LinUCB是处理Contextual Bandit的一个方法，在LinUCB中，设定&lt;/p&gt;

&lt;p&gt;\begin{align}
E\left[r_{t,a}|x_{t,a}\right] = x_{t,a}^T\theta_a
\end{align}&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta_a&lt;/script&gt;是LinUCB模型的参数，维度为&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;每个arm维护一个&lt;script type=&quot;math/tex&quot;&gt;\theta_a&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;对于单个arm，以其前m个context向量为行向量组成的矩阵称为&lt;script type=&quot;math/tex&quot;&gt;D_a&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;前m个reward组成的向量称为&lt;script type=&quot;math/tex&quot;&gt;c_a&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;使用&lt;a href=&quot;/2016/05/18/ridge-regression/&quot;&gt;ridge regression&lt;/a&gt;，可以得到&lt;script type=&quot;math/tex&quot;&gt;\theta_a&lt;/script&gt;的概率分布为高斯分布&lt;/p&gt;

&lt;p&gt;\begin{align}
\theta_a \sim N \left((D_a^TD_a + I)^{-1}D_a^Tc_a, (D_a^TD_a + I)^{-1}\right)
\end{align}&lt;/p&gt;

&lt;p&gt;为了符号简洁，令&lt;/p&gt;

&lt;p&gt;\begin{align}
\hat{\theta}_a &amp;amp;= (D_a^TD_a + I)^{-1}D_a^Tc_a \\
A_a &amp;amp;= D_a^TD_a + I
\end{align}&lt;/p&gt;

&lt;p&gt;于是&lt;script type=&quot;math/tex&quot;&gt;\theta_a&lt;/script&gt;的概率分布可表示为&lt;script type=&quot;math/tex&quot;&gt;\theta_a \sim N(\hat{\theta}_a, A_a^{-1})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;于是在第t次时可以得到&lt;script type=&quot;math/tex&quot;&gt;x_{t,a}^T\theta_a \sim N(x_{t,a}^T\hat{\theta}_a, x_{t,a}^TA_a^{-1}x_{t,a})&lt;/script&gt;，也就是&lt;script type=&quot;math/tex&quot;&gt;r_{t,a} \sim N(x_{t,a}^T\hat{\theta}_a, x_{t,a}^TA_a^{-1}x_{t,a})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;根据高斯分布的性质，其&lt;script type=&quot;math/tex&quot;&gt;1-\delta&lt;/script&gt;的置信区间为&lt;/p&gt;

&lt;p&gt;\begin{align}
\left[ x_{t,a}^T\hat{\theta}_a - \alpha\sqrt{x_{t,a}^TA_a^{-1}x_{t,a}} \quad , \quad x_{t,a}^T\hat{\theta}_a + \alpha\sqrt{x_{t,a}^TA_a^{-1}x_{t,a}}\right]
\end{align}&lt;/p&gt;

&lt;p&gt;其中&lt;script type=&quot;math/tex&quot;&gt;\alpha = 1 + \sqrt{\frac{ln(2/\delta)}{2}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;得到置信上界后就可以使用普通UCB规则了&lt;/p&gt;

&lt;p&gt;需要注意的是，&lt;script type=&quot;math/tex&quot;&gt;A_a与D_a^Tc_a&lt;/script&gt;可以增量更新，于是标准流程如下&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;设定&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;For t = 1,2,3,…
    &lt;ul&gt;
      &lt;li&gt;对所有的arm获得本次的context向量&lt;/li&gt;
      &lt;li&gt;For all &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;
        &lt;ul&gt;
          &lt;li&gt;if &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; is new
            &lt;ul&gt;
              &lt;li&gt;设置&lt;script type=&quot;math/tex&quot;&gt;A_a&lt;/script&gt;为单位矩阵&lt;/li&gt;
              &lt;li&gt;设置&lt;script type=&quot;math/tex&quot;&gt;b_a&lt;/script&gt;为&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;维0零向量&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;计算&lt;script type=&quot;math/tex&quot;&gt;\hat{\theta}_a = A_a^{-1}b_a&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;计算上界&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;选择最大上界对应的arm即&lt;script type=&quot;math/tex&quot;&gt;a_t&lt;/script&gt;，并得到对应的&lt;script type=&quot;math/tex&quot;&gt;r_t&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;更新&lt;script type=&quot;math/tex&quot;&gt;A_{a_t} = A_{a_t} + x_{t,a_t}x_{t,a_t}^T&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;更新&lt;script type=&quot;math/tex&quot;&gt;b_{a_t} = b_{a_t} + r_tx_{t,a_t}&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 19 May 2016 21:03:21 +0800</pubDate>
        <link>/2016/05/19/linucb/</link>
        <guid isPermaLink="true">/2016/05/19/linucb/</guid>
        
        
        <category>Machine&amp;nbsp;Learning</category>
        
      </item>
    
      <item>
        <title>Bayesian Linear Regression</title>
        <description>&lt;h2 id=&quot;section&quot;&gt;1. 线性回归&lt;/h2&gt;

&lt;p&gt;在一个标准的线性回归问题中，我们有&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;个数据点&lt;script type=&quot;math/tex&quot;&gt;\left\{x_1,x_2,...,x_n\right\}&lt;/script&gt;，以及对应的&lt;script type=&quot;math/tex&quot;&gt;\left\{y_1,y_2,...,y_n\right\}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;且&lt;script type=&quot;math/tex&quot;&gt;x_i \in R^{k*1}&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;y_i \in R&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;而线性回归中，给定&lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;和参数&lt;script type=&quot;math/tex&quot;&gt;\beta \in R^{k*1}&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt;的条件分布设定为&lt;/p&gt;

&lt;p&gt;\begin{align}
y_i = x_i^T\beta + \epsilon_i \\
\end{align}&lt;/p&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;\epsilon_i \sim N(0, \sigma^2)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;因此，&lt;script type=&quot;math/tex&quot;&gt;y_i \sim N(x_i^T\beta, \sigma^2)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;使用符号&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;表示一个&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;行&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;列的矩阵，第&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;个行向量代表&lt;script type=&quot;math/tex&quot;&gt;x_i^T&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;使用符号&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;表示大小为&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;的向量，第&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;个元素代表&lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;频率方法可直接得到一个解&lt;/p&gt;

&lt;p&gt;\begin{align}
\hat{\beta} = (X^TX)^{-1}X^Ty
\end{align}&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2. 共轭先验&lt;/h2&gt;

&lt;p&gt;在贝叶斯方法中，我们还需要补充先验，在那之前先整理似然函数&lt;/p&gt;

&lt;p&gt;\begin{align}
P(y|X,\beta,\sigma^2) &amp;amp;= \prod_{i=1}^nN(x_i^T\beta, \sigma^2) \\
&amp;amp;= \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^nexp\left(-\frac{\sum_{i=1}^n(y_i - x^T_i\beta)^2}{2\sigma^2}\right)\\
&amp;amp;= \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^nexp\left(-\frac{(y-X\beta)^T(y-X\beta)}{2\sigma^2}\right)
\end{align}&lt;/p&gt;

&lt;p&gt;去掉常数，有&lt;/p&gt;

&lt;p&gt;\begin{align}
P(y|X,\beta,\sigma^2) \propto \sigma^{-n}exp\left(-\frac{(y-X\beta)^T(y-X\beta)}{2\sigma^2}\right)
\end{align}&lt;/p&gt;

&lt;p&gt;将&lt;script type=&quot;math/tex&quot;&gt;(y-X\beta)^T(y-X\beta)&lt;/script&gt;改写形式&lt;/p&gt;

&lt;p&gt;\begin{align}
(y-X\beta)^T(y-X\beta) = (y-X\hat{\beta})^T(y-X\hat{\beta}) + (\beta - \hat{\beta})^T(X^TX)(\beta - \hat{\beta})
\end{align}&lt;/p&gt;

&lt;p&gt;令&lt;/p&gt;

&lt;p&gt;\begin{align}
v &amp;amp;= n - k \\
vs^2 &amp;amp;= (y-X\hat{\beta})^T(y-X\hat{\beta}) 
\end{align}&lt;/p&gt;

&lt;p&gt;于是我们得到一个新形式的似然函数&lt;/p&gt;

&lt;p&gt;\begin{align}
P(y|X,\beta,\sigma^2) &amp;amp; \propto \sigma^{-n}exp\left(-\frac{vs^2 + (\beta-\hat{\beta})^T(X^TX)(\beta-\hat{\beta})}{2\sigma^2}\right) \\
&amp;amp; \propto \sigma^{-n}exp\left(-\frac{vs^2 }{2\sigma^2}\right)exp\left(-\frac{(\beta-\hat{\beta})^T(X^TX)(\beta-\hat{\beta})}{2\sigma^2}\right) \\
&amp;amp; \propto \left[(\sigma^2)^{-v/2}exp\left(-\frac{vs^2 }{2\sigma^2}\right)\right] \left[(\sigma^2)^{-k/2}exp\left(-\frac{(\beta-\hat{\beta})^T(X^TX)(\beta-\hat{\beta})}{2\sigma^2}\right)\right]
\end{align}&lt;/p&gt;

&lt;p&gt;令&lt;script type=&quot;math/tex&quot;&gt;v_0,s_0&lt;/script&gt;为&lt;script type=&quot;math/tex&quot;&gt;v,s&lt;/script&gt;的先验值，随后为了使先验共轭，令先验的形式与似然函数对应&lt;/p&gt;

&lt;p&gt;\begin{align}
P(\beta,\sigma^2) = P(\sigma^2)P(\beta|\sigma^2)
\end{align}&lt;/p&gt;

&lt;p&gt;其中，令&lt;/p&gt;

&lt;p&gt;\begin{align}
P(\sigma^2) =Inv-Gamma(\frac{v_0}{2}, \frac{v_0s_0^2}{2}) \propto \sigma^{-(v_0+2)}exp\left( -\frac{v_0s_0^2}{2\sigma^2} \right)
\end{align}&lt;/p&gt;

&lt;p&gt;设定&lt;script type=&quot;math/tex&quot;&gt;a_0=\frac{v_0}{2},b_0=\frac{v_0s_0^2}{2}&lt;/script&gt;，于是&lt;/p&gt;

&lt;p&gt;\begin{align}
P(\sigma^2) \propto \sigma^{-(2a_0+2)}exp\left( -\frac{b_0}{\sigma^2} \right)
\end{align}&lt;/p&gt;

&lt;p&gt;而另一个条件概率设定为高斯分布，式中的&lt;script type=&quot;math/tex&quot;&gt;\mu_0,\Lambda_0&lt;/script&gt;为先验值&lt;/p&gt;

&lt;p&gt;\begin{align}
P(\beta |\sigma^2) \propto \sigma^{-k}exp\left( -\frac{(\beta-\mu_0)^T\Lambda_0(\beta - \mu_0)}{2\sigma^2} \right)
\end{align}&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;3. 后验&lt;/h2&gt;

&lt;p&gt;有了先验和似然，可以开始计算后验了，根据贝叶斯公式&lt;/p&gt;

&lt;p&gt;\begin{align}
P(\beta,\sigma^2|y,X) &amp;amp;\propto P(y|X,\beta,\sigma^2)P(\beta|\sigma^2)P(\sigma^2) \\
&amp;amp; \propto \left[\sigma^{-n}exp\left(-\frac{(y-X\beta)^T(y-X\beta)}{2\sigma^2}\right)\right]\left[\sigma^{-k}exp\left( -\frac{(\beta-\mu_0)^T\Lambda_0(\beta - \mu_0)}{2\sigma^2} \right)\right] \\
&amp;amp;* \left[\sigma^{-(2a_0+2)}exp\left( -\frac{b_0}{\sigma^2} \right)\right]
\end{align}&lt;/p&gt;

&lt;p&gt;由于&lt;/p&gt;

&lt;p&gt;\begin{align}
(y-X\beta)^T(y-X\beta) + (\beta-\mu_0)^T\Lambda_0(\beta - \mu_0) = (\beta-\mu_n)^T(X^TX+\Lambda_0)(\beta - \mu_n) + y^Ty - \mu_n^T(X^TX+\Lambda_0)\mu_n + \mu_0^T\Lambda_0\mu_0
\end{align}&lt;/p&gt;

&lt;p&gt;式中&lt;script type=&quot;math/tex&quot;&gt;\mu_n = (X^TX + \Lambda_0)^{-1}(X^TX\hat{\beta} + \Lambda_0\mu_0)&lt;/script&gt;，由上式也可以看出&lt;script type=&quot;math/tex&quot;&gt;\mu_n&lt;/script&gt;就是&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;的后验均值&lt;/p&gt;

&lt;p&gt;使用&lt;script type=&quot;math/tex&quot;&gt;\mu_n&lt;/script&gt;，可以整理后验概率如下&lt;/p&gt;

&lt;p&gt;\begin{align}
P(\beta,\sigma^2|y,X) \propto \left[\sigma^{-k}exp\left( -\frac{(\beta-\mu_n)^T(X^TX + \Lambda_0)(\beta-\mu_n)}{2\sigma^2} \right)\right] \left[\sigma^{-n-2a_0-2} exp \left( -\frac{2b_0 + y^Ty - \mu_n^T(X^TX + \Lambda_0)\mu_n+\mu_0^T\Lambda_0\mu_0}{2\sigma^2} \right)\right]
\end{align}&lt;/p&gt;

&lt;p&gt;可以看到后验概率整理成一个高斯分布&lt;script type=&quot;math/tex&quot;&gt;N(\mu_n,\sigma^2\Lambda_n^{-1})&lt;/script&gt;乘上一个逆伽马分布&lt;script type=&quot;math/tex&quot;&gt;Inv-Gamma(a_n,b_n)&lt;/script&gt;，与先验概率的形式保持一致&lt;/p&gt;

&lt;p&gt;由整理后的式子容易得到&lt;/p&gt;

&lt;p&gt;\begin{align}
\mu_n &amp;amp;= (X^TX + \Lambda_0)^{-1}(\Lambda_0\mu_0 + X^Ty) \\
\Lambda_n &amp;amp;= X^TX + \Lambda_0 \\
a_n &amp;amp;= a_0 + \frac{n}{2} \\
b_n &amp;amp;= b_0 + \frac{y^Ty+\mu_0^T\Lambda_0\mu_0-\mu_n^T\Lambda_n\mu_n}{2}
\end{align}&lt;/p&gt;

&lt;p&gt;也就是&lt;/p&gt;

&lt;p&gt;\begin{align}
P(\beta,\sigma^2|y,X) \propto \left[\sigma^{-k}exp\left( -\frac{(\beta-\mu_n)^T\Lambda_n(\beta-\mu_n)}{2\sigma^2} \right)\right] \left[(\sigma^2)^{-a_n-1} exp \left( -\frac{b_n}{\sigma^2} \right)\right]
\end{align}&lt;/p&gt;

&lt;h2 id=&quot;ridge-regression&quot;&gt;4. Ridge Regression&lt;/h2&gt;

&lt;p&gt;当&lt;script type=&quot;math/tex&quot;&gt;\mu_0=0,\Lambda_0=cI&lt;/script&gt;时称为Ridge Regression&lt;/p&gt;

&lt;p&gt;式中的&lt;script type=&quot;math/tex&quot;&gt;c \in R&lt;/script&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 18 May 2016 20:52:47 +0800</pubDate>
        <link>/2016/05/18/ridge-regression/</link>
        <guid isPermaLink="true">/2016/05/18/ridge-regression/</guid>
        
        
        <category>Machine&amp;nbsp;Learning</category>
        
      </item>
    
      <item>
        <title>Multi-Armed Bandit</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#multi-armed-bandit&quot; id=&quot;markdown-toc-multi-armed-bandit&quot;&gt;1. Multi-Armed Bandit&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#contextual-bandit&quot; id=&quot;markdown-toc-contextual-bandit&quot;&gt;2. Contextual Bandit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;multi-armed-bandit&quot;&gt;1. Multi-Armed Bandit&lt;/h2&gt;

&lt;p&gt;在赌徒面前有一排老虎机，选择任意一台老虎机后，该台老虎机会给一个数值奖励，每台老虎机给的数值奖励服从一个单独的概率分布&lt;/p&gt;

&lt;p&gt;赌徒并不知道每台老虎机的概率分布，赌徒的目标是最大化玩T次后得到的总奖励&lt;/p&gt;

&lt;p&gt;赌徒想获取各个老虎机分布的信息（哪台老虎机能获得最大的奖励），这个称为exploration&lt;/p&gt;

&lt;p&gt;同时又要根据exploration的结果获得最多的奖励，这个称为exploitation&lt;/p&gt;

&lt;p&gt;赌徒面临着exploration和exploitation的权衡，这就是Multi-Armed Bandit问题&lt;/p&gt;

&lt;p&gt;一个比较有名的方法称为&lt;script type=&quot;math/tex&quot;&gt;\epsilon-greedy&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;在某轮选择中&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用&lt;script type=&quot;math/tex&quot;&gt;1-\epsilon&lt;/script&gt;的概率选择迄今为止奖励均值最高的老虎机&lt;/li&gt;
  &lt;li&gt;使用&lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;的概率选择剩下的老虎机（剩下的老虎机被选择概率相等）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\epsilon-greedy&lt;/script&gt;兼顾了exploration和exploitation，通常&lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;设定为一个较小的值（比如0.01）&lt;/p&gt;

&lt;p&gt;实际的在线推荐系统可以直接套用这个问题&lt;/p&gt;

&lt;p&gt;在线推荐的问题中，我们也有一堆的老虎机（商品/广告等），选择一个物品，会得到一个奖励（用户购买/点击等）&lt;/p&gt;

&lt;p&gt;在线推荐的目标也是最大化这个总奖励&lt;/p&gt;

&lt;h2 id=&quot;contextual-bandit&quot;&gt;2. Contextual Bandit&lt;/h2&gt;

&lt;p&gt;在实际的在线推荐中，通常我们还会有一个特征向量描述当时的上下文场景（用户历史行为、天气等）&lt;/p&gt;

&lt;p&gt;每次决策时，需要额外利用这个特征向量的信息，这种Multi-Armed Bandit的变种称为&lt;/p&gt;

&lt;p&gt;一个处理Contextual Bandit问题的算法流程如下&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For t = 1,2,3,…
    &lt;ul&gt;
      &lt;li&gt;获取老虎机集合&lt;script type=&quot;math/tex&quot;&gt;A_t&lt;/script&gt;，对每台老虎机&lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;，获取所有的上下文向量（特征向量）&lt;script type=&quot;math/tex&quot;&gt;x_{t,a}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;选择一个老虎机&lt;script type=&quot;math/tex&quot;&gt;a_t&lt;/script&gt;作为本轮的选择，随后得到本轮的奖励&lt;script type=&quot;math/tex&quot;&gt;r_{t,a_t}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;使用新的观测值&lt;script type=&quot;math/tex&quot;&gt;(x_{t,a_t},a_t,r_{t,a_t})&lt;/script&gt;和历史的观测值调整策略&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;于是，我们可以定义T轮的总奖励为&lt;script type=&quot;math/tex&quot;&gt;\sum_{t=1}^Tr_{t,a_t}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;定义第t轮时最大奖励对应的老虎机为&lt;script type=&quot;math/tex&quot;&gt;a_t^*&lt;/script&gt;，那么可以定义最优的T轮总奖励为&lt;script type=&quot;math/tex&quot;&gt;\sum_{t=1}^Tr_{t,a_t^*}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;这个差值称为regret，定义T轮regret&lt;/p&gt;

&lt;p&gt;\begin{align}
R_A(T) = E\left[\sum_{t=1}^Tr_{t,a_t}\right]-E\left[\sum_{t=1}^Tr_{t,a_t^*}\right]
\end{align}&lt;/p&gt;

&lt;p&gt;最大化奖励，也就是最小化regret了&lt;/p&gt;

</description>
        <pubDate>Tue, 17 May 2016 01:02:11 +0800</pubDate>
        <link>/2016/05/17/mab/</link>
        <guid isPermaLink="true">/2016/05/17/mab/</guid>
        
        
        <category>Machine&amp;nbsp;Learning</category>
        
      </item>
    
      <item>
        <title>Newton Method</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1. 牛顿法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;2. 阻尼牛顿法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;3. 拟牛顿条件&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dfp&quot; id=&quot;markdown-toc-dfp&quot;&gt;4. DFP算法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bfgs&quot; id=&quot;markdown-toc-bfgs&quot;&gt;5. BFGS算法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#l-bfgs&quot; id=&quot;markdown-toc-l-bfgs&quot;&gt;6. L-BFGS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#owlqn&quot; id=&quot;markdown-toc-owlqn&quot;&gt;7. OWLQN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;1. 牛顿法&lt;/h2&gt;

&lt;p&gt;考虑问题&lt;script type=&quot;math/tex&quot;&gt;f(x) = 0&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;迭代法，选择一个接近&lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;零点的&lt;script type=&quot;math/tex&quot;&gt;x^0&lt;/script&gt;，计算相应的&lt;script type=&quot;math/tex&quot;&gt;f(x^0)&lt;/script&gt;和切线斜率&lt;script type=&quot;math/tex&quot;&gt;f^{&#39;}(x^0)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;计算穿过&lt;script type=&quot;math/tex&quot;&gt;(x^0,f(x^0))&lt;/script&gt;的切线和x轴的交点的x坐标，记为&lt;script type=&quot;math/tex&quot;&gt;x^1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;\begin{align}
x^1 = x^0 - \frac{f(x^0)}{f^{‘}(x^0)}
\end{align}&lt;/p&gt;

&lt;p&gt;通常&lt;script type=&quot;math/tex&quot;&gt;x^1&lt;/script&gt;会比&lt;script type=&quot;math/tex&quot;&gt;x^0&lt;/script&gt;更接近解，使用&lt;script type=&quot;math/tex&quot;&gt;x^1&lt;/script&gt;进行下一轮迭代，迭代公式为&lt;/p&gt;

&lt;p&gt;\begin{align}
x^{m+1} = x^{m} - \frac{f(x^m)}{f^{‘}(x^m)}
\end{align}&lt;/p&gt;

&lt;p&gt;设某个机器学习问题所求解的cost function为f(x)，那么其最优解一般满足&lt;script type=&quot;math/tex&quot;&gt;f^{&#39;}(x) = 0&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;使用牛顿法，迭代式子为&lt;/p&gt;

&lt;p&gt;\begin{align}x^{m+1} = x^{m} - \frac{f^{‘}(x^m)}{f^{‘’}(x^m)}\end{align}&lt;/p&gt;

&lt;p&gt;将上式推广到多维，有&lt;/p&gt;

&lt;p&gt;\begin{align}
x^{m+1} = x^{m} - {[Hf(x^m)]}^{-1}\nabla f(x^m)
\end{align}&lt;/p&gt;

&lt;p&gt;H为Hessian矩阵，下面谈一下上式的收敛条件&lt;/p&gt;

&lt;p&gt;\begin{align}
- {[Hf(x^m)]}^{-1}\nabla f(x^m) = x^{m+1} - x^{m}  \Longrightarrow \nabla f(x^m) = -[Hf(x^m)](x^{m+1} - x^{m})
\end{align}&lt;/p&gt;

&lt;p&gt;且&lt;/p&gt;

&lt;p&gt;\begin{align}
(x^{m+1} - x^{m})^T\nabla f(x^m) = -(x^{m+1} - x^{m})^T[Hf(x^m)](x^{m+1} - x^{m})
\end{align}&lt;/p&gt;

&lt;p&gt;当满足&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
(x^{m+1} - x^{m})^T\nabla f(x^m) &lt; 0 %]]&gt;&lt;/script&gt;时，可保证cost function在下降&lt;/p&gt;

&lt;p&gt;因此必须使得&lt;script type=&quot;math/tex&quot;&gt;(x^{m+1} - x^{m})^T[Hf(x^m)](x^{m+1} - x^{m}) &gt; 0&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;所以可以得到结论，当Hessian矩阵正定时，可保证收敛&lt;/p&gt;

&lt;p&gt;步骤总结&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;给定初值&lt;script type=&quot;math/tex&quot;&gt;x^0&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;m = 0&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算&lt;script type=&quot;math/tex&quot;&gt;\nabla f(x^m)&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;Hf(x^m)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;确定下降方向&lt;script type=&quot;math/tex&quot;&gt;d^m = - {[Hf(x^m)]}^{-1}\nabla f(x^m)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算新的x值，&lt;script type=&quot;math/tex&quot;&gt;x^{m+1}=x^m+d^m&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;令&lt;script type=&quot;math/tex&quot;&gt;m =m+ 1&lt;/script&gt;，回到第2步&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2. 阻尼牛顿法&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;给定初值&lt;script type=&quot;math/tex&quot;&gt;x^0&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;m = 0&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算&lt;script type=&quot;math/tex&quot;&gt;\nabla f(x^m)&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;Hf(x^m)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;确定下降方向&lt;script type=&quot;math/tex&quot;&gt;d^m = - {[Hf(x^m)]}^{-1}\nabla f(x^m)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算步长&lt;script type=&quot;math/tex&quot;&gt;\lambda^m = {argmin_\lambda} f(x^m + \lambda d^m)&lt;/script&gt;，计算新的x值，&lt;script type=&quot;math/tex&quot;&gt;x^{m+1}=x^m+\lambda^m d^m&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;令&lt;script type=&quot;math/tex&quot;&gt;m =m+ 1&lt;/script&gt;，回到第2步&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section-2&quot;&gt;3. 拟牛顿条件&lt;/h2&gt;

&lt;p&gt;回忆牛顿法的迭代式子&lt;/p&gt;

&lt;p&gt;\begin{align}
x^{m+1} = x^{m} - {[Hf(x^m)]}^{-1}\nabla f(x^m)
\end{align}&lt;/p&gt;

&lt;p&gt;拟牛顿法的基本思想是构造一个可以近似Hessian矩阵或者其逆矩阵的正定矩阵&lt;/p&gt;

&lt;p&gt;将&lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;在&lt;script type=&quot;math/tex&quot;&gt;x^{m+1}&lt;/script&gt;处做泰勒展开，得到&lt;/p&gt;

&lt;p&gt;\begin{align}
f(x) \approx f(x^{m+1}) + \nabla f(x^{m+1})(x-x^{m+1}) + \frac{1}{2}\nabla^2 f(x^{m+1})(x-x^{m+1})^2
\end{align}&lt;/p&gt;

&lt;p&gt;计算其梯度&lt;/p&gt;

&lt;p&gt;\begin{align}
\nabla f(x) \approx \nabla f(x^{m+1}) + [Hf(x^{m+1})](x - x^{m+1})
\end{align}&lt;/p&gt;

&lt;p&gt;为了方便，用符号&lt;script type=&quot;math/tex&quot;&gt;H^{m+1}&lt;/script&gt;表示&lt;script type=&quot;math/tex&quot;&gt;[Hf(x^{m+1})]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;于是&lt;/p&gt;

&lt;p&gt;\begin{align}
\nabla f(x) \approx \nabla f(x^{m+1}) + H^{m+1}(x - x^{m+1})
\end{align}&lt;/p&gt;

&lt;p&gt;取&lt;script type=&quot;math/tex&quot;&gt;x=x^m&lt;/script&gt;，有
\begin{align}
\nabla f(x^m) \approx \nabla f(x^{m+1}) + H^{m+1}(x^m - x^{m+1})
\end{align}&lt;/p&gt;

&lt;p&gt;整理，得&lt;/p&gt;

&lt;p&gt;\begin{align}
\nabla f(x^{m+1}) -\nabla f(x^{m})\approx  H^{m+1}(x^{m+1} - x^{m})
\end{align}&lt;/p&gt;

&lt;p&gt;迭代过程中的Hessian矩阵受上式约束，上式也称为拟牛顿条件&lt;/p&gt;

&lt;p&gt;定义符号B为拟牛顿法对Hessian矩阵的近似矩阵
定义符号D为拟牛顿法对Hessian矩阵的逆的近似矩阵&lt;/p&gt;

&lt;h2 id=&quot;dfp&quot;&gt;4. DFP算法&lt;/h2&gt;

&lt;p&gt;DFP算法的基本思想是通过迭代的方法，近似Hessian矩阵的逆，迭代式为&lt;/p&gt;

&lt;p&gt;\begin{align}
D^{m+1} = D^m + \Delta D^m
\end{align}&lt;/p&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;D_0&lt;/script&gt;一般取单位矩阵&lt;/p&gt;

&lt;p&gt;将&lt;script type=&quot;math/tex&quot;&gt;\Delta D^m&lt;/script&gt;定义为&lt;/p&gt;

&lt;p&gt;\begin{align}
\Delta D^m = \alpha uu^T + \beta vv^T
\end{align}&lt;/p&gt;

&lt;p&gt;式中&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;为待定一维向量，&lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;为待定n维向量，有&lt;/p&gt;

&lt;p&gt;\begin{align}
D^{m+1} = D^m + \alpha uu^T + \beta vv^T
\end{align}
上式乘&lt;script type=&quot;math/tex&quot;&gt;y^m = \nabla f(x^{m+1}) -\nabla f(x^{m})&lt;/script&gt;，结合拟牛顿条件有&lt;/p&gt;

&lt;p&gt;\begin{align}
x^{m+1} - x^{m} = D^my^m + \alpha uu^Ty^m + \beta vv^Ty^m
\end{align}&lt;/p&gt;

&lt;p&gt;变换形式，有&lt;/p&gt;

&lt;p&gt;\begin{align}
x^{m+1} - x^{m} = D^my^m + (\alpha u^Ty^m)u + (\beta v^Ty^m)v
\end{align}&lt;/p&gt;

&lt;p&gt;令&lt;script type=&quot;math/tex&quot;&gt;\alpha = \frac{1}{u^Ty^m}&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\beta = \frac{-1}{v^Ty^m}&lt;/script&gt;，得到&lt;/p&gt;

&lt;p&gt;\begin{align}
x^{m+1} - x^{m} -D^my^m=   u - v
\end{align}&lt;/p&gt;

&lt;p&gt;为使上式成立，令&lt;script type=&quot;math/tex&quot;&gt;u=x^{m+1} - x^{m}&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;v=D^my^m&lt;/script&gt;，因此&lt;/p&gt;

&lt;p&gt;\begin{align}
&amp;amp; \alpha = \frac{1}{u^Ty^m}=\frac{1}{(x^{m+1} - x^{m})^Ty^m} \\
&amp;amp; \beta = \frac{-1}{v^Ty^m} = \frac{-1}{(y^m)^TD^my^m}
\end{align}&lt;/p&gt;

&lt;p&gt;最终，得到&lt;/p&gt;

&lt;p&gt;\begin{align}
\Delta D^m &amp;amp;= \alpha uu^T + \beta vv^T \\
&amp;amp;= \frac{(x^{m+1} - x^{m})(x^{m+1} - x^{m})^T}{(x^{m+1} - x^{m})^Ty^m} - \frac{D^my^m(y^m)^TD^m}{(y^m)^TD^my^m}
\end{align}&lt;/p&gt;

&lt;p&gt;步骤总结&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;给定初值&lt;script type=&quot;math/tex&quot;&gt;x^0&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;D^0 = I&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;m=0&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算下降方向&lt;script type=&quot;math/tex&quot;&gt;d^m=- {[Hf(x^m)]}^{-1}\nabla f(x^m)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算步长&lt;script type=&quot;math/tex&quot;&gt;\lambda^m = {argmin_\lambda} f(x^m + \lambda d^m)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算新的x，&lt;script type=&quot;math/tex&quot;&gt;x^{m+1} = x^m + \lambda^md^m&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算&lt;script type=&quot;math/tex&quot;&gt;\nabla f(x^{m+1})&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算&lt;script type=&quot;math/tex&quot;&gt;y^m = \nabla f(x^{m+1}) -\nabla f(x^{m})&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算&lt;script type=&quot;math/tex&quot;&gt;D^{m+1} = D^m + \Delta D^m&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;令&lt;script type=&quot;math/tex&quot;&gt;m=m+1&lt;/script&gt;，回到第2步&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;bfgs&quot;&gt;5. BFGS算法&lt;/h2&gt;

&lt;p&gt;BFGS算法的基本思想是通过迭代的方法，近似Hessian矩阵，迭代式为
\begin{align}
B^{m+1} = B^m + \Delta B^m
\end{align}&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;B^0&lt;/script&gt;一般取单位矩阵，与DFP类似
\begin{align}\Delta B^m = \alpha uu^T + \beta vv^T\end{align}&lt;/p&gt;

&lt;p&gt;因此&lt;/p&gt;

&lt;p&gt;\begin{align}
B^{m+1} &amp;amp; = B^m + \Delta B^m \\
&amp;amp; = B^m + \alpha uu^T + \beta vv^T \\
\end{align}&lt;/p&gt;

&lt;p&gt;上式乘&lt;script type=&quot;math/tex&quot;&gt;s^m = (x^{m+1} - x^{m})&lt;/script&gt;，结合拟牛顿条件有&lt;/p&gt;

&lt;p&gt;\begin{align}
\nabla f(x^{m+1}) -\nabla f(x^{m}) &amp;amp; = B^ms^m + \alpha uu^Ts^m + \beta vv^Ts^m &lt;br /&gt;
\end{align}&lt;/p&gt;

&lt;p&gt;变换形式
\begin{align}
\nabla f(x^{m+1}) -\nabla f(x^{m}) &amp;amp; = B^ms^m + (\alpha u^Ts^m)u + (\beta v^Ts^m)v &lt;br /&gt;
\end{align}&lt;/p&gt;

&lt;p&gt;令&lt;script type=&quot;math/tex&quot;&gt;\alpha = \frac{1}{u^Ts^m}&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\beta = \frac{-1}{v^Ts^m}&lt;/script&gt;，有&lt;/p&gt;

&lt;p&gt;\begin{align}
\nabla f(x^{m+1}) -\nabla f(x^{m}) -B^ms^m =  u - v
\end{align}&lt;/p&gt;

&lt;p&gt;为使上式成立，令&lt;/p&gt;

&lt;p&gt;\begin{align}
u &amp;amp;= \nabla f(x^{m+1}) -\nabla f(x^{m}) \\
v &amp;amp;= B^ms^m
\end{align}&lt;/p&gt;

&lt;p&gt;综上
\begin{align}
\Delta B^m &amp;amp;= \alpha uu^T + \beta vv^T \\
&amp;amp;=\frac{uu^T}{u^Ts^m} - \frac{vv^T}{v^Ts^m} \\
&amp;amp;=\frac{uu^T}{u^T(x^{m+1} - x^{m})} - \frac{B^ms^m(B^ms^m)^T}{(B^ms^m)^T(x^{m+1} - x^{m})} \\
&amp;amp;=\frac{uu^T}{u^T(x^{m+1} - x^{m})} - \frac{B^m(x^{m+1} - x^{m})(x^{m+1} - x^{m})^TB^m}{(x^{m+1} - x^{m})^TB^m(x^{m+1} - x^{m})}
\end{align}&lt;/p&gt;

&lt;p&gt;令&lt;script type=&quot;math/tex&quot;&gt;y^m = \nabla f(x^{m+1}) -\nabla f(x^{m})&lt;/script&gt;，且有&lt;script type=&quot;math/tex&quot;&gt;s^m = (x^{m+1} - x^{m})&lt;/script&gt;，上式简化为&lt;/p&gt;

&lt;p&gt;\begin{align}
\Delta B^m = \frac{y^m(y^m)^T}{(y^m)^Ts^m} - \frac{B^ms^m(s^m)^TB^m}{(s^m)^TB^ms^m}
\end{align}&lt;/p&gt;

&lt;p&gt;因此&lt;/p&gt;

&lt;p&gt;\begin{align}
B^{m+1} = B^m +  \frac{y^m(y^m)^T}{(y^m)^Ts^m} - \frac{B^ms^m(s^m)^TB^m}{(s^m)^TB^ms^m}
\end{align}&lt;/p&gt;

&lt;p&gt;对上式求逆，得到&lt;/p&gt;

&lt;p&gt;\begin{align}
D^{m+1} = (I - \frac{s^m(y^m)^T}{(y^m)^Ts^m})D^m(I - \frac{y^m(s^m)^T}{(y^m)^Ts^m})+\frac{s^m(s^m)^T}{(y^m)^Ts^m}
\end{align}&lt;/p&gt;

&lt;p&gt;步骤总结&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;给定初值&lt;script type=&quot;math/tex&quot;&gt;x^0&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;D^0=I&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;m=0&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算下降方向&lt;script type=&quot;math/tex&quot;&gt;d^m = - D^m\nabla f(x^m)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算步长&lt;script type=&quot;math/tex&quot;&gt;\lambda^m = {argmin_\lambda} f(x^m + \lambda d^m)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算新的x，&lt;script type=&quot;math/tex&quot;&gt;x^{m+1} = x^m + \lambda^md^m&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算&lt;script type=&quot;math/tex&quot;&gt;\nabla f(x^{m+1})&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算&lt;script type=&quot;math/tex&quot;&gt;y^m = \nabla f(x^{m+1}) -\nabla f(x^{m})&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算&lt;script type=&quot;math/tex&quot;&gt;D^{m+1} = (I - \frac{s^m(y^m)^T}{(y^m)^Ts^m})D^m(I - \frac{y^m(s^m)^T}{(y^m)^Ts^m})+\frac{s^m(s^m)^T}{(y^m)^Ts^m}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;令&lt;script type=&quot;math/tex&quot;&gt;m=m+1&lt;/script&gt;，回到第2步&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;l-bfgs&quot;&gt;6. L-BFGS&lt;/h2&gt;

&lt;p&gt;BFGS的Hessian矩阵在数据维度较高时会使用极大的内存&lt;/p&gt;

&lt;p&gt;L-BFGS在BFGS的基础上做了近似，解决了这个问题&lt;/p&gt;

&lt;p&gt;首先回忆BFGS的Hessian矩阵迭代式&lt;/p&gt;

&lt;p&gt;\begin{align}
D^{m+1} = (I - \frac{s^m(y^m)^T}{(y^m)^Ts^m})D^m(I - \frac{y^m(s^m)^T}{(y^m)^Ts^m})+\frac{s^m(s^m)^T}{(y^m)^Ts^m}
\end{align}&lt;/p&gt;

&lt;p&gt;式中
\begin{align}
&amp;amp; s^m = (x^{m+1} - x^{m}) \\
&amp;amp; y^m = \nabla f(x^{m+1}) -\nabla f(x^{m})
\end{align}&lt;/p&gt;

&lt;p&gt;定义符号&lt;script type=&quot;math/tex&quot;&gt;\rho^m=\frac{1}{(y^m)^Ts^m}&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;V^m=I - \rho^my^m(s^m)^T&lt;/script&gt;
Hessian矩阵迭代式为&lt;/p&gt;

&lt;p&gt;\begin{align}
D^{m+1} = (V^m)^TD^mV^m+\rho^ms^m(s^m)^T
\end{align}&lt;/p&gt;

&lt;p&gt;由于&lt;script type=&quot;math/tex&quot;&gt;D^0&lt;/script&gt;通常为单位矩阵，因此有&lt;/p&gt;

&lt;p&gt;\begin{align}
D^1 &amp;amp;= (V^0)^TD^0V^0+\rho^0s^0(s^0)^T \\
D^2 &amp;amp;= (V^1)^TD^1V^1+\rho^1s^1(s^1)^T \\
&amp;amp;= (V^1)^T[(V^0)^TD^0V^0+\rho^0s^0(s^0)^T]V^1+\rho^1s^1(s^1)^T\\
&amp;amp;= (V^1)^T(V^0)^TD^0V^0V^1+(V^1)^T\rho^0s^0(s^0)^TV^1+\rho^1s^1(s^1)^T\\
D^3 &amp;amp;= (V^2)^TD^2V^2+\rho^2s^2(s^2)^T \\
&amp;amp;= (V^2)^T(V^1)^T(V^0)^TD^0V^0V^1V^2+(V^2)^T(V^1)^T\rho^0s^0(s^0)^TV^1V^2+(V^2)^T\rho^1s^1(s^1)^TV^2+\rho^2s^2(s^2)^T
\end{align}&lt;/p&gt;

&lt;p&gt;一般式&lt;/p&gt;

&lt;p&gt;\begin{align}
D^{m+1}=&amp;amp; [(V^m)^T(V^{m-1})^T…(V^0)^T]D^0(V^0V^1…V^m) \\
+&amp;amp;[(V^m)^T(V^{m-1})^T…(V^1)^T]\rho^0s^0(s^0)^T(V^1V^2…V^m) \\
+&amp;amp;[(V^m)^T(V^{m-1})^T…(V^2)^T]\rho^1s^1(s^1)^T(V^2V^3…V^m) \\
+ &amp;amp;… \\
+ &amp;amp;(V^m)^T\rho^{m-1}s^{m-1}(s^{m-1})^TV^m \\
+ &amp;amp;\rho^{m}s^{m}(s^{m})^T
\end{align}&lt;/p&gt;

&lt;p&gt;由上式，计算&lt;script type=&quot;math/tex&quot;&gt;D^{m+1}&lt;/script&gt;需要用到&lt;script type=&quot;math/tex&quot;&gt;\{s^i,y^i\}_{i=0}^m&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;因此，在计算过程中不存储Hessian矩阵，而是连续地存储k组&lt;script type=&quot;math/tex&quot;&gt;\{s^i, y^i\}&lt;/script&gt;，就能够精确地计算得到&lt;script type=&quot;math/tex&quot;&gt;D^1,D^2,...,D^{k}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;对于&lt;script type=&quot;math/tex&quot;&gt;D^{k+1}&lt;/script&gt;开始的计算就需要近似了，若要精确地计算&lt;script type=&quot;math/tex&quot;&gt;D^{k+1}&lt;/script&gt;，我们需要的信息是&lt;script type=&quot;math/tex&quot;&gt;\{s^i,y^i\}_{i=0}^k&lt;/script&gt;，一共k+1组数据，但只能保存k组数据，因此在进行&lt;script type=&quot;math/tex&quot;&gt;D^{k+1}&lt;/script&gt;的计算之前需要抛弃一组，因此舍弃最早的一组向量&lt;script type=&quot;math/tex&quot;&gt;\{s^0, y^0\}&lt;/script&gt;，也就是说，在计算&lt;script type=&quot;math/tex&quot;&gt;D^{k+1}&lt;/script&gt;时使用的信息为&lt;script type=&quot;math/tex&quot;&gt;\{s^i,y^i\}_{i=1}^k&lt;/script&gt;，以此类推，计算&lt;script type=&quot;math/tex&quot;&gt;D^{k+2}&lt;/script&gt;时使用的信息为&lt;script type=&quot;math/tex&quot;&gt;\{s^i,y^i\}_{i=2}^{k+1}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;得到近似计算式&lt;/p&gt;

&lt;p&gt;\begin{align}
D^{m+1}=&amp;amp; [(V^m)^T(V^{m-1})^T…(V^{m-k+1})^T]D^0(V^{m-k+1}V^{m-k+2}…V^m) \\
+&amp;amp;[(V^m)^T(V^{m-1})^T…(V^{m-k+2})^T]\rho^0s^0(s^0)^T(V^{m-k+2}V^{m-k+3}…V^m) \\
+&amp;amp;[(V^m)^T(V^{m-1})^T…(V^{m-k+3})^T]\rho^1s^1(s^1)^T(V^{m-k+3}V^{m-k+4}…V^m) \\
+ &amp;amp;… \\
+ &amp;amp;(V^m)^T\rho^{m-1}s^{m-1}(s^{m-1})^TV^m \\
+ &amp;amp;\rho^{m}s^{m}(s^{m})^T
\end{align}&lt;/p&gt;

&lt;p&gt;由于Hessian矩阵的作用是计算下降方向&lt;script type=&quot;math/tex&quot;&gt;d^m = - D^m\nabla f(x^m)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;\begin{align}
D^{m}=&amp;amp; [(V^{m-1})^T(V^{m-2})^T…(V^{m-k})^T]D^0(V^{m-k}V^{m-k+1}…V^{m-1}) \\
+&amp;amp;[(V^{m-1})^T(V^{m-2})^T…(V^{m-k+1})^T]\rho^0s^0(s^0)^T(V^{m-k+1}V^{m-k+2}…V^{m-1}) \\
+&amp;amp;[(V^{m-1})^T(V^{m-2})^T…(V^{m-k+2})^T]\rho^1s^1(s^1)^T(V^{m-k+2}V^{m-k+3}…V^{m-1}) \\
+ &amp;amp;… \\
+ &amp;amp;(V^{m-1})^T\rho^{m-2}s^{m-2}(s^{m-2})^TV^{m-1} \\
+ &amp;amp;\rho^{m-1}s^{m-1}(s^{m-1})^T
\end{align}&lt;/p&gt;

&lt;p&gt;而近似计算式中的V为n*n矩阵，直接使用近似计算式并不能达到减少内存的目的，因此有一个计算的trick，在不直接计算V的情况下得到下降方向
计算下降方向的伪代码为&lt;/p&gt;

&lt;p&gt;\begin{align}
&amp;amp; q = \nabla f(x^m) \\
&amp;amp; for \quad i=m-1,m-2,…,m-k \\
&amp;amp; \quad\quad \alpha[i]=\rho^is^iq \\
&amp;amp; \quad\quad q=q-\alpha[i]y^i \\
&amp;amp; z=\frac{(y^{m-1})^Ts^{m-1}}{(y^{m-1})^Ty^{m-1}}q \\
&amp;amp; for \quad i = m-k,m-k+1,…,m-1 \\
&amp;amp; \quad\quad \beta[i]=\rho^i(y^i)^Tz \\
&amp;amp; \quad\quad z = z + s^i(\alpha[i]-\beta[i]) \\
&amp;amp; d^m=-z
\end{align}&lt;/p&gt;

&lt;p&gt;下文称k组数据为信息队列
步骤总结&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;给定初值&lt;script type=&quot;math/tex&quot;&gt;x^0&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;D^0=I&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;m=0&lt;/script&gt;，信息队列初始化为空&lt;/li&gt;
  &lt;li&gt;根据信息队列计算下降方向&lt;script type=&quot;math/tex&quot;&gt;d^m&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算步长&lt;script type=&quot;math/tex&quot;&gt;\lambda^m=argmin_\lambda f(x^m+\lambda d^m)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算&lt;script type=&quot;math/tex&quot;&gt;x^{m+1}=x^m+\lambda^m d^m&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算&lt;script type=&quot;math/tex&quot;&gt;s^m&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;y^m&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;更新信息队列&lt;/li&gt;
  &lt;li&gt;令&lt;script type=&quot;math/tex&quot;&gt;m=m+1&lt;/script&gt;，回到第2步&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;owlqn&quot;&gt;7. OWLQN&lt;/h2&gt;

&lt;p&gt;由于L1正则在0处不可微，因此在使用L1正则的情况下不能直接使用L-BFGS算法。&lt;/p&gt;

&lt;p&gt;OWLQN便是解决这个问题的一种方法。&lt;/p&gt;

&lt;p&gt;定义sign函数&lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;\begin{align}
\sigma(x)=\left\{
\begin{aligned}
-1 &amp;amp;\quad if \;  x &amp;lt; 0\\
0 &amp;amp;\quad if \; x=0\\
1 &amp;amp; \quad if \; x &amp;gt; 0
\end{aligned}
\right.
\end{align}&lt;/p&gt;

&lt;p&gt;定义&lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;函数，该函数有一个n维向量y作为参数&lt;/p&gt;

&lt;p&gt;\begin{align}
\pi_i(x;y)=\left\{\begin{aligned}
&amp;amp; x_i \quad if \; \sigma(x_i) = \sigma(y_i) \\
&amp;amp; 0 \quad otherwise
\end{aligned}\right.
\end{align}&lt;/p&gt;

&lt;p&gt;回到优化目标&lt;/p&gt;

&lt;p&gt;\begin{align}
f(x) = l(x) + C||x||_1
\end{align}&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;l(x)&lt;/script&gt;为损失函数，C为&lt;script type=&quot;math/tex&quot;&gt;L1&lt;/script&gt;系数&lt;/p&gt;

&lt;p&gt;对于向量&lt;script type=&quot;math/tex&quot;&gt;\xi\in\{-1,0,1\}^n&lt;/script&gt;，定义&lt;/p&gt;

&lt;p&gt;\begin{align}
\Omega_\xi={x\in R^n|\pi(x;\xi)=x}
\end{align}&lt;/p&gt;

&lt;p&gt;对于任意&lt;script type=&quot;math/tex&quot;&gt;x\in \Omega_\xi&lt;/script&gt;，有&lt;/p&gt;

&lt;p&gt;\begin{align}
f(x)=l(x) + C\xi^Tx
\end{align}&lt;/p&gt;

&lt;p&gt;这是一个可使用L-BFGS求解的函数&lt;/p&gt;

&lt;p&gt;因此，OWLQN的基本思想就是每一次迭代均不跨越象限，这样就能够使用L-BFGS&lt;/p&gt;

&lt;p&gt;定义伪梯度&lt;script type=&quot;math/tex&quot;&gt;\Gamma f(x)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;\begin{align}
\Gamma_if(x)=\left\{\begin{aligned}
&amp;amp; {\partial_i}^{-}f(x) &amp;amp; if \; {\partial_i}^-f(x)&amp;gt;0\\
&amp;amp; {\partial_i}^+f(x)  &amp;amp; if \; {\partial_i}^+f(x)&amp;lt;0\\
&amp;amp; 0 &amp;amp; otherwise
\end{aligned}\right.
\end{align}&lt;/p&gt;

&lt;p&gt;式中&lt;/p&gt;

&lt;p&gt;\begin{align}
 {\partial_i}^{\pm}f(x) = \frac{\partial l(x)}{\partial x_i} + \left\{\begin{aligned}
 &amp;amp; C\sigma(x_i)  \quad &amp;amp; if \; x_i \neq 0\\
 &amp;amp; \pm C \quad &amp;amp;if \; x_i = 0
 \end{aligned}\right.
\end{align}&lt;/p&gt;

&lt;p&gt;使用伪梯度作为梯度进行计算&lt;/p&gt;

&lt;p&gt;伪梯度的合理性参考下链接的图5，6，7
http://www.cnblogs.com/vivounicorn/archive/2012/06/25/2561071.html&lt;/p&gt;

&lt;p&gt;为了保证一轮迭代后x不跨越象限，还需要&lt;/p&gt;

&lt;p&gt;\begin{align}
x^{k+1}=\pi(x^{k+1};\xi^k)
\end{align}&lt;/p&gt;

&lt;p&gt;步骤总结&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;给定初值&lt;script type=&quot;math/tex&quot;&gt;x^0&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;D^0=I&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;k=0&lt;/script&gt;，信息队列初始化为空&lt;/li&gt;
  &lt;li&gt;计算梯度下降方向&lt;script type=&quot;math/tex&quot;&gt;v^k=-\Gamma f(x^k)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;通过信息队列，计算L-BFGS的方向&lt;script type=&quot;math/tex&quot;&gt;d^k=Dv^k&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;这一步是个trick，&lt;script type=&quot;math/tex&quot;&gt;d^k=\pi(d^k;v^k)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算步长&lt;script type=&quot;math/tex&quot;&gt;\lambda^k=argmin_\lambda f(x^k+\lambda d^k)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算&lt;script type=&quot;math/tex&quot;&gt;x^{k+1}=\pi(x^k+\lambda^k d^k;x^k)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;计算&lt;script type=&quot;math/tex&quot;&gt;s^k&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;y^k&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;更新信息队列&lt;/li&gt;
  &lt;li&gt;令&lt;script type=&quot;math/tex&quot;&gt;k=k+1&lt;/script&gt;，回到第2步&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 15 May 2016 05:15:29 +0800</pubDate>
        <link>/2016/05/15/newton-method/</link>
        <guid isPermaLink="true">/2016/05/15/newton-method/</guid>
        
        
        <category>Machine&amp;nbsp;Learning</category>
        
      </item>
    
      <item>
        <title>Factorization Machine</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1. 符号定义&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#fm&quot; id=&quot;markdown-toc-fm&quot;&gt;2. 度为2的FM模型&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#fm-1&quot; id=&quot;markdown-toc-fm-1&quot;&gt;3. 度为2的FM模型的计算简化&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;4. 梯度&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dfm&quot; id=&quot;markdown-toc-dfm&quot;&gt;5. 度为d的FM模型&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;1. 符号定义&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;稀疏训练集D：&lt;script type=&quot;math/tex&quot;&gt;{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}),..., (x^{(m)}, y^{(m)})}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;第i个数据点：&lt;script type=&quot;math/tex&quot;&gt;x^{(i)}=(x^{(i)}_1, x^{(i)}_2,...,x^{(i)}_n)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;m(x^{i})&lt;/script&gt;：数据点&lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt;的不为0的feature数量&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\bar{m}_D&lt;/script&gt;：D中&lt;script type=&quot;math/tex&quot;&gt;m(x^{i})&lt;/script&gt;的均值&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;fm&quot;&gt;2. 度为2的FM模型&lt;/h2&gt;

&lt;p&gt;度为2的预测函数
\begin{align}
y = w_0 + \sum\limits_{i=1}^nw_ix_i + \sum\limits_{i=1}^n\sum\limits_{j=i+1}^n&amp;lt;v_i,v_j&amp;gt;x_ix_j
\end{align}&lt;/p&gt;

&lt;p&gt;式中的参数&lt;/p&gt;

&lt;p&gt;\begin{align}
w_0 \in R \quad w\in R \quad v_i,v_j \in R^K
\end{align}&lt;/p&gt;

&lt;p&gt;且&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;v_i, v_j&gt; %]]&gt;&lt;/script&gt;表示两个向量的点积&lt;/p&gt;

&lt;h2 id=&quot;fm-1&quot;&gt;3. 度为2的FM模型的计算简化&lt;/h2&gt;

&lt;p&gt;\begin{align}
\sum\limits_{i=1}^n\sum\limits_{j=i+1}^n&amp;lt;v_i,v_j&amp;gt;x_ix_j 
= &amp;amp;\frac{1}{2}\sum\limits_{i=1}^n\sum\limits_{j=1}^n&amp;lt;v_i,v_j&amp;gt;x_ix_j-\frac{1}{2}\sum\limits_{i=1}^n&amp;lt;v_i,v_i&amp;gt;x_ix_i \\
= &amp;amp;\frac{1}{2}\sum\limits_{i=1}^n\sum\limits_{j=1}^n\sum\limits_{k=1}^Kv_{i,k}v_{j,k}x_ix_j-\frac{1}{2}\sum\limits_{i=1}^n\sum\limits_{k=1}^Kv_{i,k}v_{i,k}x_ix_i \\
= &amp;amp;\frac{1}{2}\sum\limits_{k=1}^K\sum\limits_{i=1}^n\sum\limits_{j=1}^nv_{i,k}v_{j,k}x_ix_j-\frac{1}{2}\sum\limits_{k=1}^K\sum\limits_{i=1}^nv_{i,k}v_{i,k}x_ix_i \\
= &amp;amp;\frac{1}{2}\sum\limits_{k=1}^K \left( \left(\sum\limits_{i=1}^nv_{i,k}x_i\right)\left(\sum\limits_{j=1}^nv_{j,k}x_j\right)\right)-\frac{1}{2}\sum\limits_{k=1}^K\sum\limits_{i=1}^nv_{i,k}v_{i,k}x_ix_i \\
= &amp;amp;\frac{1}{2}\sum\limits_{k=1}^K \left(\sum\limits_{i=1}^nv_{i,k}x_i\right)^2-\frac{1}{2}\sum\limits_{k=1}^K\sum\limits_{i=1}^nv_{i,k}^2x_i^2
\end{align}&lt;/p&gt;

&lt;p&gt;于是&lt;/p&gt;

&lt;p&gt;\begin{align}
y = w_0 + \sum\limits_{i=1}^nw_ix_i + \frac{1}{2}\sum\limits_{k=1}^K \left(\sum\limits_{i=1}^nv_{i,k}x_i\right)^2-\frac{1}{2}\sum\limits_{k=1}^K\sum\limits_{i=1}^nv_{i,k}^2x_i^2
\end{align}&lt;/p&gt;

&lt;p&gt;上式的计算复杂度是O(Kn)&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;4. 梯度&lt;/h2&gt;

&lt;p&gt;\begin{align}
\frac{\partial y}{\partial w_0} &amp;amp;= 1 \\
\frac{\partial y}{\partial w_{i\geq1}} &amp;amp;= x_i \\
\frac{\partial y}{\partial v_{i,k}}&amp;amp;= x_i\left(\sum\limits_{j=1}^nv_{j,k}x_j\right) - v_{i,k}x_i^2
\end{align}&lt;/p&gt;

&lt;h2 id=&quot;dfm&quot;&gt;5. 度为d的FM模型&lt;/h2&gt;

&lt;p&gt;\begin{align}
y = w_0 + \sum\limits_{i=1}^nw_ix_i + \sum\limits_{l=2}^d\sum\limits_{i_1=1}^n…\sum\limits_{i_l=i_{l-1}+1}^n\left(\prod_{j=1}^lx_{i_j}\right)\left(\sum\limits_{k=1}^{K_l}\prod_{j=1}^lv^{(l)}_{i_j,k}\right)
\end{align}&lt;/p&gt;
</description>
        <pubDate>Sun, 15 May 2016 05:04:07 +0800</pubDate>
        <link>/2016/05/15/fm/</link>
        <guid isPermaLink="true">/2016/05/15/fm/</guid>
        
        
        <category>Machine&amp;nbsp;Learning</category>
        
      </item>
    
      <item>
        <title>Spherical Hashing for Large Scale KNN</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1. 相似性度量&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;2. 哈希函数&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;2.1 定义&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;2.2 编码长度&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-4&quot; id=&quot;markdown-toc-section-4&quot;&gt;2.3 球心与半径&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#section-5&quot; id=&quot;markdown-toc-section-5&quot;&gt;2.3.1 求解主流程&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#section-6&quot; id=&quot;markdown-toc-section-6&quot;&gt;2.3.2 调整半径——均匀划分数据&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#section-7&quot; id=&quot;markdown-toc-section-7&quot;&gt;2.3.3 调整球心——两两独立&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-8&quot; id=&quot;markdown-toc-section-8&quot;&gt;3. 查询&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-9&quot; id=&quot;markdown-toc-section-9&quot;&gt;4. 杂谈&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;1. 相似性度量&lt;/h2&gt;

&lt;p&gt;两个向量A和B的相似性度量
\begin{align}
A &amp;amp;= (a_1,a_2,a_3,…,a_n) \\
B &amp;amp;= (b_1,b_2,b_3,…,b_n)
\end{align}
相似度用符号S表示，S越大表示A和B越相似&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2. 哈希函数&lt;/h2&gt;

&lt;h3 id=&quot;section-2&quot;&gt;2.1 定义&lt;/h3&gt;
&lt;p&gt;假设二进制编码长度为&lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;位，那么需要&lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;个哈希函数
每个哈希函数代表一个超球，当样本与球心的相似系数大于等于球半径时，哈希函数的结果为&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;，否则为&lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;
设第&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;个哈希函数代表的超球的球心为&lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt;，半径为&lt;script type=&quot;math/tex&quot;&gt;r_i&lt;/script&gt;，有哈希函数&lt;script type=&quot;math/tex&quot;&gt;h_i(x)&lt;/script&gt;
\begin{align}
h_i(x)= \left\{
\begin{aligned}
&amp;amp; 0 &amp;amp; S(p_i, x) &amp;lt; r_i \\
&amp;amp; 1 &amp;amp; S(p_i, x) \geq r_i
\end{aligned}
\right.
\end{align}&lt;/p&gt;

&lt;p&gt;对于每个数据点，使用这C个哈希函数能得到长度为C的二进制编码&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;2.2 编码长度&lt;/h3&gt;

&lt;p&gt;假设用户有N个数据点，要计算每个点的K个邻近点
那么希望一个桶里平均有3K个，应该有&lt;script type=&quot;math/tex&quot;&gt;N/(3K)&lt;/script&gt;个桶
\begin{align}
C = \lfloor log_2\frac{N}{3K} \rfloor
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;2.3 球心与半径&lt;/h3&gt;

&lt;h4 id=&quot;section-5&quot;&gt;2.3.1 求解主流程&lt;/h4&gt;

&lt;p&gt;定义&lt;/p&gt;

&lt;p&gt;\begin{align}
o_i &amp;amp;= | \{ x|h_i(x)=1\}| \\
o_{i,j} &amp;amp;= | \{ x|h_i(x)=1,h_j(x)=1\}|
\end{align}&lt;/p&gt;

&lt;p&gt;理想的超球应满足&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;均匀划分数据，&lt;script type=&quot;math/tex&quot;&gt;P(h_i(x) = 1) = \frac{1}{2}&lt;/script&gt;，即&lt;script type=&quot;math/tex&quot;&gt;o_i=\frac{N}{2}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;球之间两两独立，&lt;script type=&quot;math/tex&quot;&gt;P(h_i(x) = h_j(x) = 1) = P(h_i(x))P(h_j(x))=\frac{1}{4}&lt;/script&gt;，即&lt;script type=&quot;math/tex&quot;&gt;o_{i,j}=\frac{N}{4}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由于随机子集可以近似完整数据集的分布，因此随机选取数据子集&lt;script type=&quot;math/tex&quot;&gt;\\{x_1,x_2,...,x_m\\}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;这里取 &lt;script type=&quot;math/tex&quot;&gt;m=max(\frac{N}{100}, 1e4)&lt;/script&gt;，1e4为人为设定的参数&lt;/p&gt;

&lt;p&gt;接下来以这个子集为优化目标，得到C个哈希函数&lt;/p&gt;

&lt;p&gt;算法步骤&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在子集中随机选取C个点，作为C个超球的球心&lt;/li&gt;
  &lt;li&gt;调整C个超球的半径，使得在子集满足条件『均匀划分数据』&lt;/li&gt;
  &lt;li&gt;迭代 &lt;br /&gt;
 3.1. 调整C个超球的球心使得在子集逼近条件『两两独立』&lt;br /&gt;
 3.2. 调整C个超球的半径，使得在子集满足条件『均匀划分数据』&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;section-6&quot;&gt;2.3.2 调整半径——均匀划分数据&lt;/h4&gt;

&lt;p&gt;固定球心&lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt;，求球半径&lt;script type=&quot;math/tex&quot;&gt;r_i&lt;/script&gt;，显然&lt;script type=&quot;math/tex&quot;&gt;r_i&lt;/script&gt;为&lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt;与子集中所有点相似度的中位数&lt;/p&gt;

&lt;p&gt;并行设计：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;使用M个map计算每个数据点与C个球心的相似度&lt;/li&gt;
  &lt;li&gt;Reduce，得到C个中位数作为半径&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;section-7&quot;&gt;2.3.3 调整球心——两两独立&lt;/h4&gt;

&lt;p&gt;很直观的做法&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当&lt;script type=&quot;math/tex&quot;&gt;o_{i,j}&gt;\frac{m}{4}&lt;/script&gt;时，两球应受到一个斥力&lt;/li&gt;
  &lt;li&gt;当&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
o_{i,j}&lt;\frac{m}{4} %]]&gt;&lt;/script&gt;时，两球应受到一个引力&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;定义球i受到的来自球j的力&lt;script type=&quot;math/tex&quot;&gt;f_{i\leftarrow j}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;\begin{align}
f_{i\leftarrow j}=\frac{1}{2}\frac{o_{i,j}-m/4}{m/4}(p_i-p_j)
\end{align}&lt;/p&gt;

&lt;p&gt;显然&lt;/p&gt;

&lt;p&gt;\begin{align}
f_{j\leftarrow i}=-f_{i\leftarrow j}
\end{align}&lt;/p&gt;

&lt;p&gt;并行设计：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;使用m个map，每个map处理一条数据，每条数据计算C个哈希函数，取其中哈希值为1的哈希函数ID两两配对，作为map的输出流&lt;/li&gt;
  &lt;li&gt;reduce操作，得到任意两个哈希函数i和j对应的&lt;script type=&quot;math/tex&quot;&gt;o_{i,j}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;使用&lt;script type=&quot;math/tex&quot;&gt;C^2&lt;/script&gt;个map，得到任意两个超球之间的力&lt;/li&gt;
  &lt;li&gt;reduce，得到每个超球受到的合力，并更新球心&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section-8&quot;&gt;3. 查询&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;将query转为二进制编码&lt;/li&gt;
  &lt;li&gt;用二进制编码对应的桶中的数据更新堆中的top K&lt;/li&gt;
  &lt;li&gt;如果堆大小等于K，则结束，否则进入下一步&lt;/li&gt;
  &lt;li&gt;按照球面海明距离从小到大遍历桶，直到堆的大小等于K&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;x和y为两个二进制串，其球面海明距离为:
\begin{align}
d=\frac{(x异或y)中的1的数量}{(x\&amp;amp;y)中的1的数量}
\end{align}&lt;/p&gt;

&lt;h2 id=&quot;section-9&quot;&gt;4. 杂谈&lt;/h2&gt;

&lt;p&gt;在实际工业环境中，数据通常是稀疏的，因此这个KNN方案作用有限，也只有在大公司内会有用了。&lt;/p&gt;
</description>
        <pubDate>Sun, 15 May 2016 04:01:34 +0800</pubDate>
        <link>/2016/05/15/sphd/</link>
        <guid isPermaLink="true">/2016/05/15/sphd/</guid>
        
        
        <category>Machine&amp;nbsp;Learning</category>
        
      </item>
    
      <item>
        <title>word2vec</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1. 基本符号定义&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#hierarchical-softmax&quot; id=&quot;markdown-toc-hierarchical-softmax&quot;&gt;2. Hierarchical Softmax&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cbow&quot; id=&quot;markdown-toc-cbow&quot;&gt;2.1 CBOW&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;2.1.1 预处理和符号定义&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;2.1.2 优化目标&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;2.1.3 模型&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#section-4&quot; id=&quot;markdown-toc-section-4&quot;&gt;2.1.3 梯度&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#skip-gram&quot; id=&quot;markdown-toc-skip-gram&quot;&gt;2.2 Skip-gram&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#section-5&quot; id=&quot;markdown-toc-section-5&quot;&gt;2.2.1 预处理和符号定义&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#section-6&quot; id=&quot;markdown-toc-section-6&quot;&gt;2.2.2 梯度&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#negative-sampling&quot; id=&quot;markdown-toc-negative-sampling&quot;&gt;3. Negative Sampling&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cbow-1&quot; id=&quot;markdown-toc-cbow-1&quot;&gt;3.1 CBOW&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#skip-gram-1&quot; id=&quot;markdown-toc-skip-gram-1&quot;&gt;3.2 Skip-gram&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-7&quot; id=&quot;markdown-toc-section-7&quot;&gt;3.3 负采样算法&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;1. 基本符号定义&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;w：词库中的单词;&lt;/li&gt;
  &lt;li&gt;Context(w): 特定句子中w的上下文;&lt;/li&gt;
  &lt;li&gt;v(w): w的词向量，即最终结果;&lt;/li&gt;
  &lt;li&gt;m: 词向量的维数;&lt;/li&gt;
  &lt;li&gt;C: 语料;&lt;/li&gt;
  &lt;li&gt;D: 词库。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hierarchical-softmax&quot;&gt;2. Hierarchical Softmax&lt;/h2&gt;

&lt;h3 id=&quot;cbow&quot;&gt;2.1 CBOW&lt;/h3&gt;

&lt;h4 id=&quot;section-1&quot;&gt;2.1.1 预处理和符号定义&lt;/h4&gt;

&lt;p&gt;对于语料中的每一个单词w，取在其之前的c个单词和之后的c个单词作为Context(w)，将数据保存为二元组形式(w, Context(w))。&lt;/p&gt;

&lt;p&gt;对于语料中的所有单词，按照出现频率构建哈夫曼树，使得频率较高的单词距离根节点较近，频率较低的单词距离根节点较远。&lt;/p&gt;

&lt;p&gt;对于哈夫曼树，每个叶节点与一个单词对应。&lt;/p&gt;

&lt;p&gt;对于单词w，有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;p^w&lt;/script&gt;: 从根节点出发到达w对应叶节点的路径&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;l^w&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;p^w&lt;/script&gt;中包含节点的个数&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;p_i^w&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;p^w&lt;/script&gt;的第i个节点，根节点为第1个节点，叶节点为最后一个节点&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;d_i^w&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;p^w&lt;/script&gt;中第i个节点对应的哈夫曼编码，根节点不对应编码&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-2&quot;&gt;2.1.2 优化目标&lt;/h4&gt;

&lt;p&gt;对于二元组(w, Context(w))，CBOW模型根据Context(w)预测w。&lt;/p&gt;

&lt;p&gt;优化目标为最大化对数似然函数
\begin{align}
\zeta = \sum\limits_{w\in C} \log p(w|Context(w))
\end{align}&lt;/p&gt;

&lt;h4 id=&quot;section-3&quot;&gt;2.1.3 模型&lt;/h4&gt;

&lt;p&gt;在CBOW模型中，Context(w)在CBOW模型中为2c个单词，将这2c个单词的词向量累加，得到向量&lt;script type=&quot;math/tex&quot;&gt;X_w&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;为了方便后续处理，给&lt;script type=&quot;math/tex&quot;&gt;X_w&lt;/script&gt;扩充一维，数值为1，位置在第一维。&lt;/p&gt;

&lt;p&gt;对于单词w，从根节点出发到达该单词代表的叶节点，中间经过的每一个非叶节点，都视为一个逻辑回归二分类器，分类结果对应左右子节点。&lt;/p&gt;

&lt;p&gt;约定：分到左边为0类（编码0），分到右边是1类（编码1）。&lt;/p&gt;

&lt;p&gt;哈夫曼树的叶节点为所有单词，非叶节点视为一个逻辑回归二分类器。
定义&lt;script type=&quot;math/tex&quot;&gt;\theta_i^w&lt;/script&gt;为&lt;script type=&quot;math/tex&quot;&gt;p^w&lt;/script&gt;中第i个节点对应的参数向量，不包括叶节点。
得到
\begin{align}
p(w|Context(w))=\prod\limits_{i=2}^{l^w}p(d_i^w|X_w, \theta_{i-1}^w)
\end{align}&lt;/p&gt;

&lt;p&gt;使用符号g代表sigmoid函数，有&lt;/p&gt;

&lt;p&gt;\begin{align}
p(d_i^w|X_w, \theta_{i-1}^w) = \left\{ \begin{aligned}
1-g(X_w^T\theta_{i-1}^w) \quad &amp;amp; d_i^w = 0 \\
g(X_w^T\theta_{i-1}^w) \quad &amp;amp; d_i^w = 1
\end{aligned} \right.
\end{align}&lt;/p&gt;

&lt;p&gt;为了方便，上式变形为&lt;/p&gt;

&lt;p&gt;\begin{align}
p(d_i^w|X_w, \theta_{i-1}^w)=[1-g(X_w^T\theta_{i-1}^w)]^{1-d_i^w}[g(X_w^T\theta_{i-1}^w)]^{d_i^w}
\end{align}&lt;/p&gt;

&lt;h4 id=&quot;section-4&quot;&gt;2.1.3 梯度&lt;/h4&gt;

&lt;p&gt;将上式代入到优化目标中，得到&lt;/p&gt;

&lt;p&gt;\begin{align}
\zeta=\sum\limits_{w\in C}\sum\limits_{i=2}^{l^w}{(1-d_i^w)log[1-g(x_w^T\theta_{i-1}^w)]+d_i^wlog[g(x_w^T\theta_{i-1}^w)]}
\end{align}&lt;/p&gt;

&lt;p&gt;令
\begin{align}
J={(1-d_i^w)log[1-g(x_w^T\theta_{i-1}^w)]+d_i^wlog[g(x_w^T\theta_{i-1}^w)]}
\end{align}
求导，得&lt;/p&gt;

&lt;p&gt;\begin{align}
\frac{\sigma J}{\sigma \theta_{i-1}^w} &amp;amp;= [d_i^w-g(x_w^T\theta_{i-1}^w)]X_w \\
\frac{\sigma J}{\sigma X_w} &amp;amp;= [d_i^w-g(x_w^T\theta_{i-1}^w)]\theta_{i-1}^w
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;skip-gram&quot;&gt;2.2 Skip-gram&lt;/h3&gt;

&lt;h4 id=&quot;section-5&quot;&gt;2.2.1 预处理和符号定义&lt;/h4&gt;

&lt;p&gt;同CBOW。&lt;/p&gt;

&lt;h4 id=&quot;section-6&quot;&gt;2.2.2 梯度&lt;/h4&gt;

&lt;p&gt;对于二元组(w, Context(w))，Skip-gram模型根据w预测Context(w)。&lt;/p&gt;

&lt;p&gt;优化目标为最大化对数似然函数
\begin{align}
\zeta = \sum\limits_{w\in C} \log p(Context(w)|w)
\end{align}&lt;/p&gt;

&lt;p&gt;其中&lt;/p&gt;

&lt;p&gt;\begin{align}
p(Context(w)|w) = \prod\limits_{u\in Context(w)}p(u|w)
\end{align}&lt;/p&gt;

&lt;p&gt;且&lt;/p&gt;

&lt;p&gt;\begin{align}
p(u|w)=\prod\limits_{i=2}^{l^u}p(d_i^u|v(w),\theta_{i-1}^u)=\prod\limits_{i=2}^{l^u}[1-g({v(w)}^T\theta_{i-1}^u)]^{1-d_i^u}[g({v(w)}^T\theta_{i-1}^u)]^{d_i^u}
\end{align}&lt;/p&gt;

&lt;p&gt;回代得到&lt;/p&gt;

&lt;p&gt;\begin{align}
\zeta=\sum\limits_{w\in C}\sum\limits_{u\in Context(w)}\sum\limits_{i=2}^{l^u}(1-d_i^u)log[1-g({v(w)}^T\theta_{i-1}^u)]+d_i^ulog[g({v(w)}^T\theta_{i-1}^u)]
\end{align}&lt;/p&gt;

&lt;p&gt;令&lt;/p&gt;

&lt;p&gt;\begin{align}
J = (1-d_i^u)log[1-g({v(w)}^T\theta_{i-1}^u)]+d_i^ulog[g({v(w)}^T\theta_{i-1}^u)]
\end{align}&lt;/p&gt;

&lt;p&gt;求导&lt;/p&gt;

&lt;p&gt;\begin{align}
\frac{\sigma J}{\sigma \theta_{i-1}^u} &amp;amp;= [d_i^u-g({v(w)}^T\theta_{i-1}^u)]v(w) \\
\frac{\sigma J}{\sigma v(w)} &amp;amp;= [d_i^u-g({v(w)}^T\theta_{i-1}^u)]\theta_{i-1}^u
\end{align}&lt;/p&gt;

&lt;h2 id=&quot;negative-sampling&quot;&gt;3. Negative Sampling&lt;/h2&gt;

&lt;h3 id=&quot;cbow-1&quot;&gt;3.1 CBOW&lt;/h3&gt;

&lt;p&gt;根据Context(w)预测w，对于给定的二元组(w, Context(w))，这是一个正样本，其余的词就是负样本。&lt;/p&gt;

&lt;p&gt;由于负样本数量较大，只选取其中的一个子集，具体的选取在3.3节中讲述。&lt;/p&gt;

&lt;p&gt;假设对于w有一个选取好的负样本集NEG(w)。&lt;/p&gt;

&lt;p&gt;Negative Sampling没有哈夫曼树，重新定义&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;符号。&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta^w&lt;/script&gt;为词w对应的一个辅助向量，为待训练参数。&lt;/p&gt;

&lt;p&gt;定义&lt;script type=&quot;math/tex&quot;&gt;g(X_w^T\theta^u)&lt;/script&gt;为当上下文为Context(w)时，预测中心词为u的概率。&lt;/p&gt;

&lt;p&gt;最大化目标函数为&lt;/p&gt;

&lt;p&gt;\begin{align}
\zeta=\sum\limits_{w\in C}\left\{log[g(X_w^T\theta^w)]+\sum\limits_{u\in NEG(w)}log[1-g(X_w^T\theta^u)]\right\}
\end{align}&lt;/p&gt;

&lt;p&gt;使上式最大化，也就是最大化正样本概率的同时最小化负样本概率。&lt;/p&gt;

&lt;p&gt;令&lt;/p&gt;

&lt;p&gt;\begin{align}
J=log[g(X_w^T\theta^w)]+\sum\limits_{u\in NEG(w)}log[1-g(X_w^T\theta^u)]
\end{align}&lt;/p&gt;

&lt;p&gt;求导，得&lt;/p&gt;

&lt;p&gt;\begin{align}
\frac{\sigma J}{\sigma X_w} &amp;amp;= [1-g(X_w^T\theta^w)]\theta^w-\sum\limits_{u\in NEG(w)}g(X_w^T\theta^u)\theta^u \\
\frac{\sigma J}{\sigma \theta^w} &amp;amp;=[1-g(X_w^T\theta^w)]X_w \\
\frac{\sigma J}{\sigma \theta^u} &amp;amp;= -g(X_w^T\theta^u)X_w
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;skip-gram-1&quot;&gt;3.2 Skip-gram&lt;/h3&gt;

&lt;p&gt;优化目标
\begin{align}
\zeta=\sum\limits_{w\in C}\sum\limits_{u\in Context(w)}\left\{log[g({v(w)}^T\theta^u)]+\sum\limits_{z\in NEG(u)}log[1-g({v(w)}^T\theta^z)]\right\}
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;3.3 负采样算法&lt;/h3&gt;

&lt;p&gt;带权采样，语料中的频率越大，词的选取概率越大。
Google的word2vec中的概率为&lt;/p&gt;

&lt;p&gt;\begin{align}
\frac{counter(w)^{\frac{3}{4}}}{\sum\limits_{u\in D}counter(u)^{\frac{3}{4}}}
\end{align}&lt;/p&gt;

</description>
        <pubDate>Sun, 15 May 2016 01:09:47 +0800</pubDate>
        <link>/2016/05/15/word2vec/</link>
        <guid isPermaLink="true">/2016/05/15/word2vec/</guid>
        
        
        <category>Machine&amp;nbsp;Learning</category>
        
      </item>
    
      <item>
        <title>Isolation Forest</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#isolation-tree&quot; id=&quot;markdown-toc-isolation-tree&quot;&gt;1. Isolation Tree&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#characteristic-of-isolation-trees&quot; id=&quot;markdown-toc-characteristic-of-isolation-trees&quot;&gt;2. Characteristic of Isolation Trees&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#anomaly-detection-using-iforest&quot; id=&quot;markdown-toc-anomaly-detection-using-iforest&quot;&gt;3. Anomaly Detection using iForest&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#training&quot; id=&quot;markdown-toc-training&quot;&gt;3.1 Training&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#evaluating-stage&quot; id=&quot;markdown-toc-evaluating-stage&quot;&gt;3.2 Evaluating Stage&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;isolation-tree&quot;&gt;1. Isolation Tree&lt;/h2&gt;

&lt;p&gt;用T表示Isolation Tree的一个节点，如果T是非叶节点，则一定有两个子节点&lt;script type=&quot;math/tex&quot;&gt;(T_l,T_r)&lt;/script&gt;，且记录了一个特征&lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt;和一个值&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;，特征&lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt;的值小于p的数据点划分到&lt;script type=&quot;math/tex&quot;&gt;T_l&lt;/script&gt;，否则划分到&lt;script type=&quot;math/tex&quot;&gt;T_r&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;用h(x)表示数据点x落到的叶节点距离根节点的距离
设定有n个数据点，数据点是d维
使用c(n)表示h(x)的均值&lt;/p&gt;

&lt;p&gt;\begin{align}
c(n)=2H(n-1)-\frac{2(n-1)}{n}
\end{align}&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;H(i)&lt;/script&gt;是谐波数
\begin{align}
H(i)\approx \ln(i)+0.5772156649
\end{align}&lt;/p&gt;

&lt;p&gt;Anomaly分数定义为&lt;/p&gt;

&lt;p&gt;\begin{align}
s(x,n)=2^{-\frac{E(h(x))}{c(n)}}
\end{align}&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;E(h(x))&lt;/script&gt;是多个iTree的&lt;script type=&quot;math/tex&quot;&gt;h(x)&lt;/script&gt;的均值&lt;/p&gt;

&lt;p&gt;因此&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果数据点的&lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;接近1，判定为异常点&lt;/li&gt;
  &lt;li&gt;如果数据点的&lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;小于0.5，可以认为是非常正常的数据点&lt;/li&gt;
  &lt;li&gt;如果所有数据点的&lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;都在0.5附近，那么不存在任何异常点&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;characteristic-of-isolation-trees&quot;&gt;2. Characteristic of Isolation Trees&lt;/h2&gt;

&lt;p&gt;异常点密集处导致&lt;script type=&quot;math/tex&quot;&gt;h(x)&lt;/script&gt;偏大，iForest难于检测
因此需要采样降低异常点的密度，相比于全量构造，更快且效果更好。&lt;/p&gt;

&lt;h2 id=&quot;anomaly-detection-using-iforest&quot;&gt;3. Anomaly Detection using iForest&lt;/h2&gt;

&lt;h3 id=&quot;training&quot;&gt;3.1 Training&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Input: 输入数据&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;，森林大小&lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;，采样大小&lt;script type=&quot;math/tex&quot;&gt;\varphi&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Output: 森林&lt;/li&gt;
  &lt;li&gt;步骤：
    &lt;ul&gt;
      &lt;li&gt;初始化&lt;/li&gt;
      &lt;li&gt;设定树高限制&lt;script type=&quot;math/tex&quot;&gt;l=ceiling(\log_2\varphi)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;迭代t次
        &lt;ul&gt;
          &lt;li&gt;取样生成子集&lt;script type=&quot;math/tex&quot;&gt;X&#39;&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;生成新的iTree&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;限制树高，是因为我们只关心小于平均高度的节点。
根据经验值，&lt;script type=&quot;math/tex&quot;&gt;\varphi=256，t=100&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;evaluating-stage&quot;&gt;3.2 Evaluating Stage&lt;/h3&gt;

&lt;p&gt;令&lt;script type=&quot;math/tex&quot;&gt;e&lt;/script&gt;为实际长度
由于限制了树高，实际计算&lt;script type=&quot;math/tex&quot;&gt;h(x)&lt;/script&gt;时
\begin{align}
h(x)=e+c(\varphi)
\end{align}&lt;/p&gt;
</description>
        <pubDate>Sun, 15 May 2016 00:24:11 +0800</pubDate>
        <link>/2016/05/15/iforest/</link>
        <guid isPermaLink="true">/2016/05/15/iforest/</guid>
        
        
        <category>Machine&amp;nbsp;Learning</category>
        
      </item>
    
  </channel>
</rss>
