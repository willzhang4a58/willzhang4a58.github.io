<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>willzhang4a58&#39;s Personal Website</title>
    <description>willzhang4a58&#39;s Personal Website</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 19 Feb 2016 23:21:55 +0800</pubDate>
    <lastBuildDate>Fri, 19 Feb 2016 23:21:55 +0800</lastBuildDate>
    <generator>Jekyll v3.1.1</generator>
    
      <item>
        <title>Logistic Regression</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1. 前言&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;2. 符号定义&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;3. 优化目标&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;4. 梯度&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reference&quot; id=&quot;markdown-toc-reference&quot;&gt;5. Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;1. 前言&lt;/h2&gt;

&lt;p&gt;Logistic Regression是业界最常见的分类算法之一。因其原理简单，且效果不错，被大量地应用于实际业务中。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2. 符号定义&lt;/h2&gt;

&lt;p&gt;设有如下m个数据点
\begin{align}
{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), …, (x^{(m)}, y^{(m)})}
\end{align}&lt;/p&gt;

&lt;p&gt;每个数据点包含特征x和类别y，其中
\begin{align}
x^{(i)}&amp;amp;=[x^{(i)}_1, x^{(i)}_2, …, x^{(i)}_n]^T \\
y^{(i)}&amp;amp; \in \{0, 1\}
\end{align}&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;3. 优化目标&lt;/h2&gt;

&lt;p&gt;Logistic Regression的目标是寻找一个函数完成从x到y的映射，如下
\begin{align}
h(x) = sigmoid(w^Tx+b)
\end{align}&lt;/p&gt;

&lt;p&gt;w和b为Logistic Regression的参数，通过调整这两项参数使得预测的类别h(x)更贴近真实的类别的y&lt;br /&gt;
通常，对h(x)的概率解释是x对应的y为1的概率，因此1 - h(x)也就是对应的y为0的概率&lt;br /&gt;
因此，其似然函数为&lt;/p&gt;

&lt;p&gt;\begin{align}
L(w,b)=\prod_{i=1}^m{h(x^{(i)})}^{y^{(i)}}(1-h(x^{(i)}))^{1-y^{(i)}}
\end{align}&lt;/p&gt;

&lt;p&gt;Logistic Regression的优化目标就是似然函数最大化&lt;/p&gt;

&lt;p&gt;实际应用中通常对似然函数取对数&lt;/p&gt;

&lt;p&gt;\begin{align}
\ln L(w,b)&amp;amp;=\sum_{i=1}^my^{(i)}\ln {h(x^{(i)})} + (1-y^{(i)})ln {(1-h(x^{(i)}))}
\end{align}&lt;/p&gt;

&lt;p&gt;再乘-1则为最后的Cost Function&lt;/p&gt;

&lt;p&gt;\begin{align}
Cost&amp;amp;=\sum_{i=1}^m-y^{(i)}\ln {h(x^{(i)})} - (1-y^{(i)})ln {(1-h(x^{(i)}))}
\end{align}&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;4. 梯度&lt;/h2&gt;

&lt;p&gt;无论是梯度下降，还是牛顿法，都需要计算梯度，因此本文也求解一下&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;求梯度&lt;/p&gt;

&lt;p&gt;\begin{align}
\frac{\partial Cost}{\partial h(x^{(i)})} &amp;amp;= -\frac{y^{(i)}}{h(x^{(i)})} - \frac{(1-y^{(i)})}{h(x^{(i)}) - 1} \\
\frac{\partial h(x^{(i)})}{\partial w} &amp;amp;= sigmoid’(w^Tx^{(i)}+b)x^{(i)} = h(x^{(i)})(1-h(x^{(i)}))x^{(i)}\\
\frac{\partial h(x^{(i)})}{\partial b} &amp;amp;= sigmoid’(w^Tx^{(i)}+b) = h(x^{(i)})(1-h(x^{(i)})) \\
\frac{\partial Cost}{\partial w} &amp;amp;= \sum_{i=1}^m\frac{\partial Cost}{\partial h(x^{(i)})}\frac{\partial h(x^{(i)})}{\partial w} = \sum_{i=1}^m  y^{(i)}(h(x^{(i)}) -1)x^{(i)} + (1-y^{(i)})  h(x^{(i)})x^{(i)} = \sum_{i=1}^m (h(x^{(i)})-y^{(i)})x^{(i)}  \\
\frac{\partial Cost}{\partial b} &amp;amp;= \sum_{i=1}^m\frac{\partial Cost}{\partial h(x^{(i)})}\frac{\partial h(x^{(i)})}{\partial b} = \sum_{i=1}^m  y^{(i)}(h(x^{(i)}) -1) + (1-y^{(i)})  h(x^{(i)}) = \sum_{i=1}^m h(x^{(i)})-y^{(i)}
\end{align}&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;5. Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;Wiki——Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Fri, 19 Feb 2016 21:47:24 +0800</pubDate>
        <link>/2016/02/19/logistic-regression/</link>
        <guid isPermaLink="true">/2016/02/19/logistic-regression/</guid>
        
        
        <category>Machine&amp;nbsp;Learning</category>
        
      </item>
    
  </channel>
</rss>
