---
layout: post
title:  "LinUCB"
date: 2016-05-16 16:03:42
categories: Machine&nbsp;Learning
comments: true
---

* content
{:toc}

## 1. UCB

[上一篇文章]中介绍了一种context-free的方法，$$\epsilon-greedy$$。

其能够很好地完成对未知商品的exploration，同时完成对奖励的exploitation。

但是在后期，假如所有的商品都已经探索完毕，并且没有新商品加进来

显然此时并不需要做exploration，$$\epsilon$$概率对应部分的奖励就拿不到了

在此，介绍一种新的方法，Upper Confidence Bound

这里仅简单介绍一下基本思想

假如有一个老虎机，玩了10次，奖励是2，那么可以认为其奖励均值是2，此时设定其均值的95%置信区间为[0,4]

而另一个老虎机，玩了100词，奖励均值是3，那么可以设定其均值的95%置信区间为[2.9,3.1]

UCB在此时的决策是选择置信区间上界最大的一个老虎机

很容易发现

* 对于未知商品，尽管其均值可能很低，但是由于其不确定性会导致置信区间的上界会很大，从而触发exploration
* 对于已经很熟悉的商品，如果其均值很高，会触发exploitation

UCB的难点在于如何设定置信上界，设定方法有很多种，这里不介绍

设定的方法通常会满足如下性质

* 随着迭代轮数的增长，上界会远离均值
* 随着被选择次数的增长，上界会靠近均值

## 2. LinUCB

### 2.1 Disjoint

[上一篇文章]中提到了Contextual Bandit，在Multi-Armed Bandit的基础上每次做决策时还会有一个特征向量

LinUCB是处理Contextual Bandit的一个方法，在LinUCB中，设定

\begin{align}
E\left\[r_{t,a}|x_{t,a}\right] = x_{t,a}^T\theta_a^*
\end{align}

$$\theta_a^*$$是LinUCB模型的参数

每个arm维护一个$$\theta_a^*$$，因此这个称为Disjoint Model

对于单个arm，以其前m个context向量为行向量组成的矩阵称为$$D_a$$

前m个reward组成的向量称为$$c_a$$

使用ridge regression，可以得到一个解$$\hat{\theta}_a$$		

\begin{align}
\hat{\theta}_a = (D_a^TD_a + I_d)^{-1}D_a^Tc_a
\end{align}

[上一篇文章]: /2016/05/16/mab
